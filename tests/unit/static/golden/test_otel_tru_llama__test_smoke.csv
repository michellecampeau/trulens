,record,event_id,record_attributes,record_type,resource_attributes,start_timestamp,timestamp,trace
0,"{'name': 'root', 'kind': 1, 'parent_span_id': '', 'status': 'STATUS_CODE_UNSET'}",11212156408139769493,"{'name': 'root', 'trulens.span_type': 'record_root', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '9c92625d-607e-4b51-af8f-a372054bc471', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.record_root.app_name': 'Simple RAG', 'trulens.record_root.app_version': 'v1', 'trulens.record_root.record_id': '9c92625d-607e-4b51-af8f-a372054bc471'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 20:08:30.930607,2025-01-22 20:08:31.393687,"{'trace_id': '127851412367517904361571005073597672767', 'parent_id': '', 'span_id': '11212156408139769493'}"
1,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'kind': 1, 'parent_span_id': '11212156408139769493', 'status': 'STATUS_CODE_UNSET'}",15329803488001043951,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'trulens.span_type': 'main', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '9c92625d-607e-4b51-af8f-a372054bc471', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.main.main_input': 'What is multi-headed attention?', 'trulens.main.main_output': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 20:08:30.930658,2025-01-22 20:08:31.393670,"{'trace_id': '127851412367517904361571005073597672767', 'parent_id': '11212156408139769493', 'span_id': '15329803488001043951'}"
2,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'kind': 1, 'parent_span_id': '15329803488001043951', 'status': 'STATUS_CODE_UNSET'}",3839990740897793843,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '9c92625d-607e-4b51-af8f-a372054bc471', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.query_bundle': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 20:08:30.930785,2025-01-22 20:08:31.379514,"{'trace_id': '127851412367517904361571005073597672767', 'parent_id': '15329803488001043951', 'span_id': '3839990740897793843'}"
3,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'kind': 1, 'parent_span_id': '3839990740897793843', 'status': 'STATUS_CODE_UNSET'}",10438019272902376529,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '9c92625d-607e-4b51-af8f-a372054bc471', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.str_or_query_bundle': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 20:08:30.930806,2025-01-22 20:08:31.379454,"{'trace_id': '127851412367517904361571005073597672767', 'parent_id': '3839990740897793843', 'span_id': '10438019272902376529'}"
4,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'kind': 1, 'parent_span_id': '10438019272902376529', 'status': 'STATUS_CODE_UNSET'}",7073795317791348661,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '9c92625d-607e-4b51-af8f-a372054bc471', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.query_bundle': 'What is multi-headed attention?'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 20:08:30.930864,2025-01-22 20:08:31.379146,"{'trace_id': '127851412367517904361571005073597672767', 'parent_id': '10438019272902376529', 'span_id': '7073795317791348661'}"
5,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'kind': 1, 'parent_span_id': '15329803488001043951', 'status': 'STATUS_CODE_UNSET'}",2869287258710407797,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '9c92625d-607e-4b51-af8f-a372054bc471', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.query_str': 'What is multi-headed attention?', 'trulens.unknown.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).', 'page_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.', 'page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 20:08:31.379783,2025-01-22 20:08:31.393367,"{'trace_id': '127851412367517904361571005073597672767', 'parent_id': '15329803488001043951', 'span_id': '2869287258710407797'}"
6,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'kind': 1, 'parent_span_id': '2869287258710407797', 'status': 'STATUS_CODE_UNSET'}",17913119017584159195,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '9c92625d-607e-4b51-af8f-a372054bc471', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.query_str': 'What is multi-headed attention?', 'trulens.unknown.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.'], 'trulens.unknown.prev_response': 'None'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 20:08:31.392238,2025-01-22 20:08:31.393310,"{'trace_id': '127851412367517904361571005073597672767', 'parent_id': '2869287258710407797', 'span_id': '17913119017584159195'}"
7,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'kind': 1, 'parent_span_id': '17913119017584159195', 'status': 'STATUS_CODE_UNSET'}",18413984140376399271,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '9c92625d-607e-4b51-af8f-a372054bc471', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.formatted': True, 'trulens.unknown.args': '(\'Context information is below.\\n---------------------\\npage_label: 4\\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\\n\\nScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\\n\\npage_label: 5\\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n\\npage_label: 2\\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\\n\\nThis makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: What is multi-headed attention?\\nAnswer: \',)', 'trulens.unknown.kwargs': ""{'formatted': True}""}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 20:08:31.392977,2025-01-22 20:08:31.393203,"{'trace_id': '127851412367517904361571005073597672767', 'parent_id': '17913119017584159195', 'span_id': '18413984140376399271'}"
