,record,event_id,record_attributes,record_type,resource_attributes,start_timestamp,timestamp,trace
0,"{'name': 'root', 'kind': 1, 'parent_span_id': '', 'status': 'STATUS_CODE_UNSET'}",14871827740061592550,"{'name': 'root', 'ai_observability.span_type': 'record_root', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'f64a0c6c-0010-48f2-9992-ee023a17ba19', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.record_root.app_name': 'Simple RAG', 'ai_observability.record_root.app_version': 'v1', 'ai_observability.record_root.record_id': 'f64a0c6c-0010-48f2-9992-ee023a17ba19'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 23:13:30.760987,2025-01-28 23:13:30.770707,"{'trace_id': '88885284068820637094476655668391634437', 'parent_id': '', 'span_id': '14871827740061592550'}"
1,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'kind': 1, 'parent_span_id': '14871827740061592550', 'status': 'STATUS_CODE_UNSET'}",906756051382029990,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'ai_observability.span_type': 'main', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'f64a0c6c-0010-48f2-9992-ee023a17ba19', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.main.main_input': 'What is multi-headed attention?', 'ai_observability.main.main_output': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 23:13:30.761050,2025-01-28 23:13:30.770696,"{'trace_id': '88885284068820637094476655668391634437', 'parent_id': '14871827740061592550', 'span_id': '906756051382029990'}"
2,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'kind': 1, 'parent_span_id': '906756051382029990', 'status': 'STATUS_CODE_UNSET'}",5105356537257209832,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'f64a0c6c-0010-48f2-9992-ee023a17ba19', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.query_bundle': 'What is multi-headed attention?', 'ai_observability.unknown.return': ['Node ID: d6fb0a58-d39a-4f34-8a66-ce6fe49b31ac\nText: Recent work has achieved significant improvements in\ncomputational efficiency through factorization tricks [21] and\nconditional computation [32], while also improving model performance\nin case of the latter. The fundamental constraint of sequential\ncomputation, however, remains. Attention mechanisms have become an\nintegral part of compelling seq...\nScore:  0.900\n', 'Node ID: a7359722-19b8-4639-ac2d-599a0ef5141f\nText: To facilitate these residual connections, all sub-layers in the\nmodel, as well as the embedding layers, produce outputs of dimension\ndmodel = 512. Decoder: The decoder is also composed of a stack of N =\n6identical layers. In addition to the two sub-layers in each encoder\nlayer, the decoder inserts a third sub-layer, which performs multi-\nhead att...\nScore:  0.891\n', 'Node ID: 4e3faf77-8fa9-4ca6-8d3d-125bad2be024\nText: Recurrent models typically factor computation along the symbol\npositions of the input and output sequences. Aligning the positions to\nsteps in computation time, they generate a sequence of hidden states\nht, as a function of the previous hidden state ht−1 and the input for\nposition t. This inherently sequential nature precludes\nparallelization wi...\nScore:  0.880\n']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 23:13:30.761180,2025-01-28 23:13:30.762505,"{'trace_id': '88885284068820637094476655668391634437', 'parent_id': '906756051382029990', 'span_id': '5105356537257209832'}"
3,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'kind': 1, 'parent_span_id': '5105356537257209832', 'status': 'STATUS_CODE_UNSET'}",12520774755217835839,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'f64a0c6c-0010-48f2-9992-ee023a17ba19', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.str_or_query_bundle': 'What is multi-headed attention?', 'ai_observability.unknown.return': ['Node ID: d6fb0a58-d39a-4f34-8a66-ce6fe49b31ac\nText: Recent work has achieved significant improvements in\ncomputational efficiency through factorization tricks [21] and\nconditional computation [32], while also improving model performance\nin case of the latter. The fundamental constraint of sequential\ncomputation, however, remains. Attention mechanisms have become an\nintegral part of compelling seq...\nScore:  0.900\n', 'Node ID: a7359722-19b8-4639-ac2d-599a0ef5141f\nText: To facilitate these residual connections, all sub-layers in the\nmodel, as well as the embedding layers, produce outputs of dimension\ndmodel = 512. Decoder: The decoder is also composed of a stack of N =\n6identical layers. In addition to the two sub-layers in each encoder\nlayer, the decoder inserts a third sub-layer, which performs multi-\nhead att...\nScore:  0.891\n', 'Node ID: 4e3faf77-8fa9-4ca6-8d3d-125bad2be024\nText: Recurrent models typically factor computation along the symbol\npositions of the input and output sequences. Aligning the positions to\nsteps in computation time, they generate a sequence of hidden states\nht, as a function of the previous hidden state ht−1 and the input for\nposition t. This inherently sequential nature precludes\nparallelization wi...\nScore:  0.880\n']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 23:13:30.761204,2025-01-28 23:13:30.762335,"{'trace_id': '88885284068820637094476655668391634437', 'parent_id': '5105356537257209832', 'span_id': '12520774755217835839'}"
4,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'kind': 1, 'parent_span_id': '12520774755217835839', 'status': 'STATUS_CODE_UNSET'}",4459057439596500886,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'f64a0c6c-0010-48f2-9992-ee023a17ba19', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.query_bundle': 'What is multi-headed attention?', 'ai_observability.unknown.return': ['Node ID: d6fb0a58-d39a-4f34-8a66-ce6fe49b31ac\nText: Recent work has achieved significant improvements in\ncomputational efficiency through factorization tricks [21] and\nconditional computation [32], while also improving model performance\nin case of the latter. The fundamental constraint of sequential\ncomputation, however, remains. Attention mechanisms have become an\nintegral part of compelling seq...\nScore:  0.900\n', 'Node ID: a7359722-19b8-4639-ac2d-599a0ef5141f\nText: To facilitate these residual connections, all sub-layers in the\nmodel, as well as the embedding layers, produce outputs of dimension\ndmodel = 512. Decoder: The decoder is also composed of a stack of N =\n6identical layers. In addition to the two sub-layers in each encoder\nlayer, the decoder inserts a third sub-layer, which performs multi-\nhead att...\nScore:  0.891\n', 'Node ID: 4e3faf77-8fa9-4ca6-8d3d-125bad2be024\nText: Recurrent models typically factor computation along the symbol\npositions of the input and output sequences. Aligning the positions to\nsteps in computation time, they generate a sequence of hidden states\nht, as a function of the previous hidden state ht−1 and the input for\nposition t. This inherently sequential nature precludes\nparallelization wi...\nScore:  0.880\n']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 23:13:30.761273,2025-01-28 23:13:30.762110,"{'trace_id': '88885284068820637094476655668391634437', 'parent_id': '12520774755217835839', 'span_id': '4459057439596500886'}"
5,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'kind': 1, 'parent_span_id': '906756051382029990', 'status': 'STATUS_CODE_UNSET'}",11639057137759579457,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'f64a0c6c-0010-48f2-9992-ee023a17ba19', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.query_str': 'What is multi-headed attention?', 'ai_observability.unknown.text_chunks': ['page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].', 'page_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.', 'page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai_observability.unknown.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 23:13:30.762594,2025-01-28 23:13:30.770543,"{'trace_id': '88885284068820637094476655668391634437', 'parent_id': '906756051382029990', 'span_id': '11639057137759579457'}"
6,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'kind': 1, 'parent_span_id': '11639057137759579457', 'status': 'STATUS_CODE_UNSET'}",3767325042109649622,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'f64a0c6c-0010-48f2-9992-ee023a17ba19', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.query_str': 'What is multi-headed attention?', 'ai_observability.unknown.text_chunks': ['page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.'], 'ai_observability.unknown.prev_response': 'None', 'ai_observability.unknown.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 23:13:30.769836,2025-01-28 23:13:30.770508,"{'trace_id': '88885284068820637094476655668391634437', 'parent_id': '11639057137759579457', 'span_id': '3767325042109649622'}"
7,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'kind': 1, 'parent_span_id': '3767325042109649622', 'status': 'STATUS_CODE_UNSET'}",4207105682006674346,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'f64a0c6c-0010-48f2-9992-ee023a17ba19', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.formatted': True, 'ai_observability.unknown.args': ['Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '], 'ai_observability.unknown.kwargs': ""{'formatted': True}"", 'ai_observability.unknown.return': 'Context information is below.\n---------------------\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19].\n\npage_label: 3\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 23:13:30.770311,2025-01-28 23:13:30.770438,"{'trace_id': '88885284068820637094476655668391634437', 'parent_id': '3767325042109649622', 'span_id': '4207105682006674346'}"
