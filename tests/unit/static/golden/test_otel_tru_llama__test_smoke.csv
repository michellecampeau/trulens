,record,event_id,record_attributes,record_type,resource_attributes,start_timestamp,timestamp,trace
0,"{'name': 'root', 'kind': 1, 'parent_span_id': '', 'status': 'STATUS_CODE_UNSET'}",4743359143934755312,"{'name': 'root', 'ai_observability.span_type': 'record_root', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'e4b16093-135c-417b-9fb2-a985dd1b849c', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.record_root.app_name': 'Simple RAG', 'ai_observability.record_root.app_version': 'v1', 'ai_observability.record_root.record_id': 'e4b16093-135c-417b-9fb2-a985dd1b849c'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 15:53:47.101450,2025-01-28 15:53:47.364857,"{'trace_id': '187392957213858972159242213853085699929', 'parent_id': '', 'span_id': '4743359143934755312'}"
1,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'kind': 1, 'parent_span_id': '4743359143934755312', 'status': 'STATUS_CODE_UNSET'}",15853655379192139064,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'ai_observability.span_type': 'main', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'e4b16093-135c-417b-9fb2-a985dd1b849c', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.main.main_input': 'What is multi-headed attention?', 'ai_observability.main.main_output': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 15:53:47.101499,2025-01-28 15:53:47.364840,"{'trace_id': '187392957213858972159242213853085699929', 'parent_id': '4743359143934755312', 'span_id': '15853655379192139064'}"
2,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'kind': 1, 'parent_span_id': '15853655379192139064', 'status': 'STATUS_CODE_UNSET'}",17595807175838123293,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'e4b16093-135c-417b-9fb2-a985dd1b849c', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.query_bundle': 'What is multi-headed attention?', 'ai_observability.unknown.return': ['Node ID: 00703848-e966-4ffb-8d5f-532fde5fc0b0\nText: Scaled Dot-Product Attention  Multi-Head Attention Figure 2:\n(left) Scaled Dot-Product Attention. (right) Multi-Head Attention\nconsists of several attention layers running in parallel. of the\nvalues, where the weight assigned to each value is computed by a\ncompatibility function of the query with the corresponding key. 3.2.1\nScaled Dot-Product A...\nScore:  0.856\n', 'Node ID: 2932ac1d-79f4-4d40-8cb5-911b00a776b1\nText: output values. These are concatenated and once again projected,\nresulting in the final values, as depicted in Figure 2. Multi-head\nattention allows the model to jointly attend to information from\ndifferent representation subspaces at different positions. With a\nsingle attention head, averaging inhibits this.\nScore:  0.854\n', 'Node ID: e7d7c9d2-b952-41a2-a0eb-f2f3330e9f15\nText: This makes it more difficult to learn dependencies between\ndistant positions [ 12]. In the Transformer this is reduced to a\nconstant number of operations, albeit at the cost of reduced effective\nresolution due to averaging attention-weighted positions, an effect we\ncounteract with Multi-Head Attention as described in section 3.2.\nSelf-attention,...\nScore:  0.848\n']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 15:53:47.101615,2025-01-28 15:53:47.348041,"{'trace_id': '187392957213858972159242213853085699929', 'parent_id': '15853655379192139064', 'span_id': '17595807175838123293'}"
3,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'kind': 1, 'parent_span_id': '17595807175838123293', 'status': 'STATUS_CODE_UNSET'}",11833885163527739356,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'e4b16093-135c-417b-9fb2-a985dd1b849c', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.str_or_query_bundle': 'What is multi-headed attention?', 'ai_observability.unknown.return': ['Node ID: 00703848-e966-4ffb-8d5f-532fde5fc0b0\nText: Scaled Dot-Product Attention  Multi-Head Attention Figure 2:\n(left) Scaled Dot-Product Attention. (right) Multi-Head Attention\nconsists of several attention layers running in parallel. of the\nvalues, where the weight assigned to each value is computed by a\ncompatibility function of the query with the corresponding key. 3.2.1\nScaled Dot-Product A...\nScore:  0.856\n', 'Node ID: 2932ac1d-79f4-4d40-8cb5-911b00a776b1\nText: output values. These are concatenated and once again projected,\nresulting in the final values, as depicted in Figure 2. Multi-head\nattention allows the model to jointly attend to information from\ndifferent representation subspaces at different positions. With a\nsingle attention head, averaging inhibits this.\nScore:  0.854\n', 'Node ID: e7d7c9d2-b952-41a2-a0eb-f2f3330e9f15\nText: This makes it more difficult to learn dependencies between\ndistant positions [ 12]. In the Transformer this is reduced to a\nconstant number of operations, albeit at the cost of reduced effective\nresolution due to averaging attention-weighted positions, an effect we\ncounteract with Multi-Head Attention as described in section 3.2.\nSelf-attention,...\nScore:  0.848\n']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 15:53:47.101633,2025-01-28 15:53:47.347691,"{'trace_id': '187392957213858972159242213853085699929', 'parent_id': '17595807175838123293', 'span_id': '11833885163527739356'}"
4,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'kind': 1, 'parent_span_id': '11833885163527739356', 'status': 'STATUS_CODE_UNSET'}",9131942659230873734,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'e4b16093-135c-417b-9fb2-a985dd1b849c', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.query_bundle': 'What is multi-headed attention?', 'ai_observability.unknown.return': ['Node ID: 00703848-e966-4ffb-8d5f-532fde5fc0b0\nText: Scaled Dot-Product Attention  Multi-Head Attention Figure 2:\n(left) Scaled Dot-Product Attention. (right) Multi-Head Attention\nconsists of several attention layers running in parallel. of the\nvalues, where the weight assigned to each value is computed by a\ncompatibility function of the query with the corresponding key. 3.2.1\nScaled Dot-Product A...\nScore:  0.856\n', 'Node ID: 2932ac1d-79f4-4d40-8cb5-911b00a776b1\nText: output values. These are concatenated and once again projected,\nresulting in the final values, as depicted in Figure 2. Multi-head\nattention allows the model to jointly attend to information from\ndifferent representation subspaces at different positions. With a\nsingle attention head, averaging inhibits this.\nScore:  0.854\n', 'Node ID: e7d7c9d2-b952-41a2-a0eb-f2f3330e9f15\nText: This makes it more difficult to learn dependencies between\ndistant positions [ 12]. In the Transformer this is reduced to a\nconstant number of operations, albeit at the cost of reduced effective\nresolution due to averaging attention-weighted positions, an effect we\ncounteract with Multi-Head Attention as described in section 3.2.\nSelf-attention,...\nScore:  0.848\n']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 15:53:47.101688,2025-01-28 15:53:47.347094,"{'trace_id': '187392957213858972159242213853085699929', 'parent_id': '11833885163527739356', 'span_id': '9131942659230873734'}"
5,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'kind': 1, 'parent_span_id': '15853655379192139064', 'status': 'STATUS_CODE_UNSET'}",6407077403407170667,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'e4b16093-135c-417b-9fb2-a985dd1b849c', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.query_str': 'What is multi-headed attention?', 'ai_observability.unknown.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).', 'page_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.', 'page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.'], 'ai_observability.unknown.return': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 15:53:47.348305,2025-01-28 15:53:47.364551,"{'trace_id': '187392957213858972159242213853085699929', 'parent_id': '15853655379192139064', 'span_id': '6407077403407170667'}"
6,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'kind': 1, 'parent_span_id': '6407077403407170667', 'status': 'STATUS_CODE_UNSET'}",3405244141599062020,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'e4b16093-135c-417b-9fb2-a985dd1b849c', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.query_str': 'What is multi-headed attention?', 'ai_observability.unknown.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.'], 'ai_observability.unknown.prev_response': 'None', 'ai_observability.unknown.return': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 15:53:47.363259,2025-01-28 15:53:47.364490,"{'trace_id': '187392957213858972159242213853085699929', 'parent_id': '6407077403407170667', 'span_id': '3405244141599062020'}"
7,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'kind': 1, 'parent_span_id': '3405244141599062020', 'status': 'STATUS_CODE_UNSET'}",3720121035299212361,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'ai_observability.span_type': 'unknown', 'ai_observability.domain': 'module', 'ai_observability.app_name': 'Simple RAG', 'ai_observability.app_version': 'v1', 'ai_observability.record_id': 'e4b16093-135c-417b-9fb2-a985dd1b849c', 'ai_observability.run_name': 'test run', 'ai_observability.input_id': '42', 'ai_observability.unknown.formatted': True, 'ai_observability.unknown.args': ['Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '], 'ai_observability.unknown.kwargs': ""{'formatted': True}"", 'ai_observability.unknown.return': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-28 15:53:47.364115,2025-01-28 15:53:47.364373,"{'trace_id': '187392957213858972159242213853085699929', 'parent_id': '3405244141599062020', 'span_id': '3720121035299212361'}"
