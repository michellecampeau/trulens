,record,event_id,record_attributes,record_type,resource_attributes,start_timestamp,timestamp,trace
0,"{'name': 'root', 'kind': 1, 'parent_span_id': '', 'status': 'STATUS_CODE_UNSET'}",17159932595256566260,"{'name': 'root', 'trulens.span_type': 'record_root', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '540e71e4-9428-4294-b7b4-1aadc5778c94', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.record_root.app_name': 'Simple RAG', 'trulens.record_root.app_version': 'v1', 'trulens.record_root.record_id': '540e71e4-9428-4294-b7b4-1aadc5778c94'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 14:57:20.344581,2025-01-22 14:57:21.185996,"{'trace_id': '113388130726100188921452836378787216765', 'parent_id': '', 'span_id': '17159932595256566260'}"
1,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'kind': 1, 'parent_span_id': '17159932595256566260', 'status': 'STATUS_CODE_UNSET'}",16836412768485789691,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'trulens.span_type': 'main', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '540e71e4-9428-4294-b7b4-1aadc5778c94', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.main.main_input': 'What is multi-headed attention?', 'trulens.main.main_output': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 14:57:20.344627,2025-01-22 14:57:21.185978,"{'trace_id': '113388130726100188921452836378787216765', 'parent_id': '17159932595256566260', 'span_id': '16836412768485789691'}"
2,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'kind': 1, 'parent_span_id': '16836412768485789691', 'status': 'STATUS_CODE_UNSET'}",11716100444579463546,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '540e71e4-9428-4294-b7b4-1aadc5778c94', 'trulens.run_name': 'test run', 'trulens.input_id': '42'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 14:57:20.344748,2025-01-22 14:57:21.169470,"{'trace_id': '113388130726100188921452836378787216765', 'parent_id': '16836412768485789691', 'span_id': '11716100444579463546'}"
3,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'kind': 1, 'parent_span_id': '11716100444579463546', 'status': 'STATUS_CODE_UNSET'}",4038847531497022398,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '540e71e4-9428-4294-b7b4-1aadc5778c94', 'trulens.run_name': 'test run', 'trulens.input_id': '42'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 14:57:20.344768,2025-01-22 14:57:21.169408,"{'trace_id': '113388130726100188921452836378787216765', 'parent_id': '11716100444579463546', 'span_id': '4038847531497022398'}"
4,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'kind': 1, 'parent_span_id': '4038847531497022398', 'status': 'STATUS_CODE_UNSET'}",10419179750587072671,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '540e71e4-9428-4294-b7b4-1aadc5778c94', 'trulens.run_name': 'test run', 'trulens.input_id': '42'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 14:57:20.344888,2025-01-22 14:57:21.169124,"{'trace_id': '113388130726100188921452836378787216765', 'parent_id': '4038847531497022398', 'span_id': '10419179750587072671'}"
5,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'kind': 1, 'parent_span_id': '16836412768485789691', 'status': 'STATUS_CODE_UNSET'}",16140204072842166305,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '540e71e4-9428-4294-b7b4-1aadc5778c94', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.query_str': 'What is multi-headed attention?', 'trulens.unknown.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).', 'page_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.', 'page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 14:57:21.169765,2025-01-22 14:57:21.185683,"{'trace_id': '113388130726100188921452836378787216765', 'parent_id': '16836412768485789691', 'span_id': '16140204072842166305'}"
6,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'kind': 1, 'parent_span_id': '16140204072842166305', 'status': 'STATUS_CODE_UNSET'}",2984073123446208401,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '540e71e4-9428-4294-b7b4-1aadc5778c94', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.query_str': 'What is multi-headed attention?', 'trulens.unknown.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 14:57:21.184447,2025-01-22 14:57:21.185627,"{'trace_id': '113388130726100188921452836378787216765', 'parent_id': '16140204072842166305', 'span_id': '2984073123446208401'}"
7,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'kind': 1, 'parent_span_id': '2984073123446208401', 'status': 'STATUS_CODE_UNSET'}",5397414478918628966,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': '540e71e4-9428-4294-b7b4-1aadc5778c94', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.formatted': True, 'trulens.unknown.args': ['Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-22 14:57:21.185264,2025-01-22 14:57:21.185500,"{'trace_id': '113388130726100188921452836378787216765', 'parent_id': '2984073123446208401', 'span_id': '5397414478918628966'}"
