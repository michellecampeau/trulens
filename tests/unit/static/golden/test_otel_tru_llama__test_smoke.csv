,record,event_id,record_attributes,record_type,resource_attributes,start_timestamp,timestamp,trace
0,"{'name': 'root', 'kind': 1, 'parent_span_id': '', 'status': 'STATUS_CODE_UNSET'}",7197281636660566312,"{'name': 'root', 'trulens.span_type': 'record_root', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': 'eaaf6587-51cf-49e6-9c6f-03c80b28b8a5', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.record_root.app_name': 'Simple RAG', 'trulens.record_root.app_version': 'v1', 'trulens.record_root.record_id': 'eaaf6587-51cf-49e6-9c6f-03c80b28b8a5'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-21 00:20:59.471316,2025-01-21 00:20:59.679359,"{'trace_id': '339779830908651296780171478275335225682', 'parent_id': '', 'span_id': '7197281636660566312'}"
1,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'kind': 1, 'parent_span_id': '7197281636660566312', 'status': 'STATUS_CODE_UNSET'}",16323642377359125052,"{'name': 'llama_index.core.base.base_query_engine.BaseQueryEngine.query', 'trulens.span_type': 'main', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': 'eaaf6587-51cf-49e6-9c6f-03c80b28b8a5', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.main.main_input': 'What is multi-headed attention?', 'trulens.main.main_output': 'Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: '}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-21 00:20:59.471374,2025-01-21 00:20:59.679347,"{'trace_id': '339779830908651296780171478275335225682', 'parent_id': '7197281636660566312', 'span_id': '16323642377359125052'}"
2,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'kind': 1, 'parent_span_id': '16323642377359125052', 'status': 'STATUS_CODE_UNSET'}",13817558135954012624,"{'name': 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine.retrieve', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': 'eaaf6587-51cf-49e6-9c6f-03c80b28b8a5', 'trulens.run_name': 'test run', 'trulens.input_id': '42'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-21 00:20:59.471501,2025-01-21 00:20:59.676066,"{'trace_id': '339779830908651296780171478275335225682', 'parent_id': '16323642377359125052', 'span_id': '13817558135954012624'}"
3,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'kind': 1, 'parent_span_id': '13817558135954012624', 'status': 'STATUS_CODE_UNSET'}",11592357232085318973,"{'name': 'llama_index.core.base.base_retriever.BaseRetriever.retrieve', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': 'eaaf6587-51cf-49e6-9c6f-03c80b28b8a5', 'trulens.run_name': 'test run', 'trulens.input_id': '42'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-21 00:20:59.471527,2025-01-21 00:20:59.676002,"{'trace_id': '339779830908651296780171478275335225682', 'parent_id': '13817558135954012624', 'span_id': '11592357232085318973'}"
4,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'kind': 1, 'parent_span_id': '11592357232085318973', 'status': 'STATUS_CODE_UNSET'}",1144918585443041336,"{'name': 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever._retrieve', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': 'eaaf6587-51cf-49e6-9c6f-03c80b28b8a5', 'trulens.run_name': 'test run', 'trulens.input_id': '42'}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-21 00:20:59.471623,2025-01-21 00:20:59.675722,"{'trace_id': '339779830908651296780171478275335225682', 'parent_id': '11592357232085318973', 'span_id': '1144918585443041336'}"
5,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'kind': 1, 'parent_span_id': '16323642377359125052', 'status': 'STATUS_CODE_UNSET'}",2535902529273907866,"{'name': 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine.get_response', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': 'eaaf6587-51cf-49e6-9c6f-03c80b28b8a5', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.query_str': 'What is multi-headed attention?', 'trulens.unknown.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).', 'page_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.', 'page_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-21 00:20:59.676345,2025-01-21 00:20:59.679014,"{'trace_id': '339779830908651296780171478275335225682', 'parent_id': '16323642377359125052', 'span_id': '2535902529273907866'}"
6,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'kind': 1, 'parent_span_id': '2535902529273907866', 'status': 'STATUS_CODE_UNSET'}",5440431783236657962,"{'name': 'llama_index.core.response_synthesizers.refine.Refine.get_response', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': 'eaaf6587-51cf-49e6-9c6f-03c80b28b8a5', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.query_str': 'What is multi-headed attention?', 'trulens.unknown.text_chunks': ['page_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-21 00:20:59.677662,2025-01-21 00:20:59.678954,"{'trace_id': '339779830908651296780171478275335225682', 'parent_id': '2535902529273907866', 'span_id': '5440431783236657962'}"
7,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'kind': 1, 'parent_span_id': '5440431783236657962', 'status': 'STATUS_CODE_UNSET'}",70272475172139449,"{'name': 'llama_index.core.llms.mock.MockLLM.complete', 'trulens.span_type': 'unknown', 'trulens.app_name': 'Simple RAG', 'trulens.app_version': 'v1', 'trulens.record_id': 'eaaf6587-51cf-49e6-9c6f-03c80b28b8a5', 'trulens.run_name': 'test run', 'trulens.input_id': '42', 'trulens.unknown.formatted': True, 'trulens.unknown.args': ['Context information is below.\n---------------------\npage_label: 4\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).\n\npage_label: 5\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n\npage_label: 2\nfile_path: tests/unit/data/attention_is_all_you_need.pdf\n\nThis makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is multi-headed attention?\nAnswer: ']}",EventRecordType.SPAN,"{'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.28.2', 'service.name': 'trulens'}",2025-01-21 00:20:59.678550,2025-01-21 00:20:59.678822,"{'trace_id': '339779830908651296780171478275335225682', 'parent_id': '5440431783236657962', 'span_id': '70272475172139449'}"
