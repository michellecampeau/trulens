{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Evaluation - evaluating your LLM-as-judge with TruLens\n",
    "\n",
    "Meta evaluation is the process of evaluating evaluation methods themselves. Here we are measuring and benchmarking the performance of LLM-based evaluators (aka LLM-as-judge), where the main focus of performance is human alignment. In other words, how closely aligned the generated scores are with human evaluation processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "In TruLens, we implement this as a special case of GroundTruth evaluation, since we canonically regard human preferences as the groundtruth in most LLM tasks. \n",
    "\n",
    "For experiment tracking, we provide a suite automatic metric computation via Aggregator, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevance feedback function\n",
    "from trulens_eval.feedback import GroundTruthAgreement, GroundTruthAggregator\n",
    "from trulens_eval import Tru\n",
    "import numpy as np\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "golden_set = [\n",
    "    {\n",
    "        \"query\": \"who are the Apple's competitors?\",\n",
    "        \"response\": \"Apple competitors include Samsung, Google, and Microsoft.\",\n",
    "        \"expected_score\": 1.0,  # groundtruth score annotated by human\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"what is the capital of France?\",\n",
    "        \"response\": \"Paris is the capital of France.\",\n",
    "        \"expected_score\": 1.0,\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"what is the capital of Spain?\",\n",
    "        \"response\": \"I love going to Spain.\",\n",
    "        \"expected_score\": 0,\n",
    "    },\n",
    "]\n",
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth = GroundTruthAgreement(golden_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback import Cortex, OpenAI\n",
    "\n",
    "# provider = Cortex(model_engine=\"mistral-large\")\n",
    "provider = OpenAI(model_engine=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# output is feedback_score\n",
    "def context_relevance_ff_to_score(input, output, benchmark_params) -> float:\n",
    "    return provider.context_relevance(\n",
    "        question=input,\n",
    "        context=output,\n",
    "        temperature=benchmark_params[\"temperature\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# output is (feedback_score, confidence_score)\n",
    "def context_relevance_ff_to_score_with_confidence(\n",
    "    input, output, benchmark_params\n",
    ") -> Tuple[float, float]:\n",
    "    return provider.context_relevance_verb_confidence(\n",
    "        question=input,\n",
    "        context=output,\n",
    "        temperature=benchmark_params[\"temperature\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all prompt and expected responses from the golden set and pass to GroundTruthAggregator as ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "responses = []\n",
    "for i in range(len(golden_set)):\n",
    "    prompt = golden_set[i][\"query\"]\n",
    "    response = golden_set[i][\"response\"]\n",
    "\n",
    "    prompts.append(prompt)\n",
    "    responses.append(response)\n",
    "\n",
    "true_labels = [entry[\"expected_score\"] for entry in golden_set]\n",
    "\n",
    "mae_agg_func = GroundTruthAggregator(true_labels=true_labels).mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.benchmark_frameworks.tru_benchmark_experiment import (\n",
    "    BenchmarkParams,\n",
    ")\n",
    "\n",
    "tru_benchmark_arctic = tru.BenchmarkExperiment(\n",
    "    app_id=\"MAE\",\n",
    "    ground_truth=golden_set,\n",
    "    trace_to_score_fn=context_relevance_ff_to_score,\n",
    "    agg_funcs=[mae_agg_func],\n",
    "    benchmark_params=BenchmarkParams(temperature=0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_benchmark_arctic as recording:\n",
    "    feedback_res = tru_benchmark_arctic.app.collect_feedback_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: compare the generated feedback scores with the passed in ground truth labels [1, 1, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_res  # generate feedback scores from our context relevance feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_gt_aggr_fnc = GroundTruthAggregator(true_labels=true_labels)\n",
    "custom_gt_aggr_fnc.custom_aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_agg_func = GroundTruthAggregator(true_labels=true_labels).ece\n",
    "tru_benchmark_arctic_calibration = tru.BenchmarkExperiment(\n",
    "    app_id=\"Expected Calibration Error (ECE)\",\n",
    "    ground_truth=golden_set,\n",
    "    trace_to_score_fn=context_relevance_ff_to_score_with_confidence,\n",
    "    agg_funcs=[ece_agg_func],\n",
    "    benchmark_params=BenchmarkParams(temperature=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_benchmark_arctic_calibration as recording:\n",
    "    feedback_results = (\n",
    "        tru_benchmark_arctic_calibration.app.collect_feedback_scores()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_results  # a tuple of (generate_feedback_scores, confidence_scores)  from our context relevance feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
