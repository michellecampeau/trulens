{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Relevance Evaluations\n",
    "\n",
    "In many ways, feedbacks can be thought of as LLM apps themselves. Given text, they return some result. Thinking in this way, we can use TruLens to evaluate and track our feedback quality. We can even do this for different models (e.g. gpt-3.5 and gpt-4) or prompting schemes (such as chain-of-thought reasoning).\n",
    "\n",
    "This notebook follows an evaluation of a set of test cases. You are encouraged to run this on your own and even expand the test cases to evaluate performance on test cases applicable to your scenario or domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevance feedback function\n",
    "from trulens_eval.feedback import GroundTruthAgreement, OpenAI as fOpenAI\n",
    "from trulens_eval import TruBasicApp, Feedback, Tru, Select\n",
    "from test_cases import context_relevance_golden_set\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = fOpenAI(model_engine=\"gpt-3.5-turbo\")\n",
    "# Define your feedback functions\n",
    "def wrapped_relevance_turbo(input, output):\n",
    "    return turbo.qs_relevance(input, output)\n",
    "\n",
    "# Define your feedback functions\n",
    "def wrapped_relevance_with_cot_turbo(input, output):\n",
    "    return turbo.qs_relevance_with_cot_reasons(input, output)\n",
    "\n",
    "gpt4 = fOpenAI(model_engine=\"gpt-4\")\n",
    "# Define your feedback functions\n",
    "def wrapped_relevance_gpt4(input, output):\n",
    "    return gpt4.qs_relevance(input, output)\n",
    "\n",
    "# Define your feedback functions\n",
    "def wrapped_relevance_with_cot_gpt4(input, output):\n",
    "    return gpt4.qs_relevance_with_cot_reasons(input, output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll set up our golden set as a set of prompts, responses and expected scores stored in `test_cases.py`. Then, our numeric_difference method will look up the expected score for each prompt/response pair by **exact match**. After looking up the expected score, we will then take the L1 difference between the actual score and expected score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Context Relevance Smoke Test, input prompt will be set to *.__record__.calls[0].args.args[0] .\n",
      "âœ… In Context Relevance Smoke Test, input response will be set to *.__record__.calls[0].args.args[1] .\n",
      "âœ… In Context Relevance Smoke Test, input score will be set to *.__record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth = GroundTruthAgreement(context_relevance_golden_set)\n",
    "# Call the numeric_difference method with app and record\n",
    "f_groundtruth = Feedback(ground_truth.numeric_difference, name = \"Context Relevance Smoke Test\").on(Select.Record.calls[0].args.args[0]).on(Select.Record.calls[0].args.args[1]).on_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "tru_wrapped_relevance_turbo = TruBasicApp(wrapped_relevance_turbo, app_id = \"context relevance gpt-3.5-turbo\", feedbacks=[f_groundtruth])\n",
    "tru_wrapped_relevance_with_cot_turbo = TruBasicApp(wrapped_relevance_with_cot_turbo, app_id = \"context relevance with cot reasoning gpt-3.5-turbo\", feedbacks=[f_groundtruth])\n",
    "tru_wrapped_relevance_gpt4 = TruBasicApp(wrapped_relevance_gpt4, app_id = \"context relevance gpt-4\", feedbacks=[f_groundtruth])\n",
    "tru_wrapped_relevance_with_cot_gpt4 = TruBasicApp(wrapped_relevance_with_cot_gpt4, app_id = \"context relevance with cot reasoning gpt-4\", feedbacks=[f_groundtruth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task queue full. Finishing existing tasks.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(context_relevance_golden_set)):\n",
    "    prompt = context_relevance_golden_set[i][\"query\"]\n",
    "    response = context_relevance_golden_set[i][\"response\"]\n",
    "    tru_wrapped_relevance_turbo.call_with_record(prompt, response)\n",
    "    tru_wrapped_relevance_with_cot_turbo.call_with_record(prompt, response)\n",
    "    tru_wrapped_relevance_gpt4.call_with_record(prompt, response)\n",
    "    tru_wrapped_relevance_with_cot_gpt4.call_with_record(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context Relevance Smoke Test</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context relevance gpt-3.5-turbo</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context relevance gpt-4</th>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.015268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context relevance with cot reasoning gpt-4</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.019560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context relevance with cot reasoning gpt-3.5-turbo</th>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Context Relevance Smoke Test  \\\n",
       "app_id                                                                             \n",
       "context relevance gpt-3.5-turbo                                         0.800000   \n",
       "context relevance gpt-4                                                 0.780000   \n",
       "context relevance with cot reasoning gpt-4                              0.733333   \n",
       "context relevance with cot reasoning gpt-3.5-turbo                      0.706667   \n",
       "\n",
       "                                                     latency  total_cost  \n",
       "app_id                                                                    \n",
       "context relevance gpt-3.5-turbo                     0.066667    0.000762  \n",
       "context relevance gpt-4                             0.066667    0.015268  \n",
       "context relevance with cot reasoning gpt-4          0.066667    0.019560  \n",
       "context relevance with cot reasoning gpt-3.5-turbo  0.066667    0.000918  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tru().get_leaderboard(app_ids=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
