{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cortex Finetuning Experiments\n",
    "\n",
    "This notebook takes you through evaluating a series of fine-tuning experiments for labelling customer support tickets with an LLM.\n",
    "\n",
    "To get your account details for this class, first go to https://sfedu02-tmb89584.snowflakecomputing.com/\n",
    "\n",
    "Log in with a chosen username/password from the distributed sign-up sheet, and then reset your password.\n",
    "\n",
    "Next, fill in those details into the connection parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "\n",
    "connection_params = {\n",
    "    \"account\": \"TMB89584\",\n",
    "    \"user\": \"INSTRUCTOR1\",\n",
    "    \"password\": \"...\",\n",
    "    \"role\": \"TRAINING_ROLE\",\n",
    "    \"database\": \"SUPPORT_TICKET_CLASSIFICATION_DB\",\n",
    "    \"schema\": \"SUPPORT_TICKETS_SCHEMA\",\n",
    "    \"warehouse\": \"ANIMAL_TASK_WH\",\n",
    "}\n",
    "\n",
    "# Create a Snowflake session\n",
    "snowpark_session = Session.builder.configs(connection_params).create()\n",
    "\n",
    "# for connecting to the database, we will also use snowflake.connector\n",
    "import snowflake.connector\n",
    "snowflake_connection = snowflake.connector.connect(**connection_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to start a TruLens logging session.\n",
    "\n",
    "In the first homework, we logged the traces and evaluations of our LLM app locally. In this homework, we will log the traces and evals into our Snowflake database. We can do that with the same connection parameters as above, except using a new schema to hold the data. You should append the schema name to hold the logs with your initials so that you can store your logs separately from the other students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "\n",
    "connection_params['schema'] = 'TRULENS_LOGS_JR'\n",
    "\n",
    "connector = SnowflakeConnector(**connection_params)\n",
    "\n",
    "session = TruSession(connector=connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our LLM App\n",
    "\n",
    "The next thing we need to do is draft our instructions for the LLM app. The goal of this app is to automatically label customer support tickets as they come in so they can be properly triaged.\n",
    "\n",
    "Each support ticket should receive one of the following five labels:\n",
    "- Roaming fees\n",
    "- Slow data speed\n",
    "- Lost phone\n",
    "- Add new line\n",
    "- Closing account\n",
    "\n",
    "Try writing an instruction prompt yourself, or use the one we've written for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = \"\"\"\n",
    "        You are an agent that helps organize requests that come to our support team. \n",
    "\n",
    "        The request category is the reason why the customer reached out. These are the possible types of request categories:\n",
    "\n",
    "        Roaming fees\n",
    "        Slow data speed\n",
    "        Lost phone\n",
    "        Add new line\n",
    "        Closing account\n",
    "\n",
    "        Try doing it for this request and return only the request category only.\n",
    "        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to use this instruciton prompt in our LLM app. This app will first render a full prompt with the instruction and ticket, and then pass the rendered prompt to an LLM.\n",
    "\n",
    "In the first homework, we used OpenAI as our LLM. Here we'll use models from Mistral, accessed via Snowflake Cortex. First, we'll try using Mistral 7b as it is the cheapest and smallest model available in this model family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next:\n",
    "\n",
    "- Create ground truth with distribution missing roaming fees label\n",
    "- Create train/test set with distribution missing roaming fees label\n",
    "- Test base model on ground truth, observe failure\n",
    "- fine-tune base model with train set - roaming fees label\n",
    "- test fine-tuned model against test set missing roaming fees label\n",
    "- observe success!\n",
    "- simulate drift:\n",
    "- test fine-tuned abse model against test set with roaming fees label\n",
    "- notice failure with trulens\n",
    "- fine-tune with additional examples of roaming fees data\n",
    "- test against full set with trulens\n",
    "- success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.cortex import Complete\n",
    "from trulens.apps.custom import instrument\n",
    "\n",
    "class Support_Ticket_Classifier:\n",
    "\n",
    "    @instrument\n",
    "    def __init__(self, model, instruction_prompt):\n",
    "        self.model = model\n",
    "        self.instruction_prompt = instruction_prompt\n",
    "        \n",
    "    @instrument\n",
    "    def classify_ticket(self, ticket):\n",
    "        rendered_prompt = self.instruction_prompt + ticket\n",
    "        label = Complete(self.model, rendered_prompt)\n",
    "        return label, rendered_prompt\n",
    "    \n",
    "support_ticket_classifier = Support_Ticket_Classifier(\"mistral-7b\", instruction_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the our test set with ground truth labels for testing.\n",
    "\n",
    "Along the way, we'll also store it in our TruLens database for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cursor object\n",
    "cursor = snowflake_connection.cursor()\n",
    "\n",
    "# Define the SQL query to fetch the data\n",
    "query = \"SELECT * FROM SUPPORT_TICKET_CLASSIFICATION_DB.SUPPORT_TICKET_SCHEMA.SUPPORT_TICKETS_GROUND_TRUTH_NO_ROAMING_FEES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(query)\n",
    "ground_truth = cursor.fetch_pandas_all()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "\n",
    "ground_truth.rename(columns={'TICKET_ID': 'query_id', 'REQUEST': 'query', 'LABEL': 'expected_response'},\n",
    "                    inplace=True)\n",
    "\n",
    "# persist data in trulens database so we can fetch it from here in the future\n",
    "session.add_ground_truth_to_dataset(\n",
    "    dataset_name=\"support_ticket_eval_groundtruth\",\n",
    "    ground_truth_df=ground_truth,\n",
    "    dataset_metadata={\"split\": \"eval\"},\n",
    ")\n",
    "\n",
    "ground_truth = session.get_ground_truth(\"support_ticket_eval_groundtruth\")\n",
    "\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create an evaluator to test against that ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.cortex import Cortex\n",
    "\n",
    "provider = Cortex(snowflake_connection, model_engine=\"mistral-large\")\n",
    "\n",
    "f_groundtruth = (\n",
    "    Feedback(\n",
    "    GroundTruthAgreement(ground_truth, provider = provider).agreement_measure,\n",
    "    name=\"Ground Truth (semantic similarity measurement)\"\n",
    "    )\n",
    "    .on_input_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create our application wrapper to track metadata and package the app with the evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_support_ticket_classifier = TruCustomApp(\n",
    "    support_ticket_classifier,\n",
    "    app_name=\"Support Ticket Classifier\",\n",
    "    app_version=\"mistral 7b\",\n",
    "    feedbacks=[f_groundtruth]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in ground_truth['query']:\n",
    "    with tru_support_ticket_classifier as recording:\n",
    "        label, rendered_prompt = support_ticket_classifier.classify_ticket(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see bad performance on our test set.\n",
    "\n",
    "In many cases, base mistral 7b is failing to follow instructions and provide only the support ticket label.\n",
    "\n",
    "We have already tried prompting to solve this issue. Now, we can turn to fine-tuning.\n",
    "\n",
    "To fine-tune the model, we can use Cortex fine-tuning to further train the base model on labeled data, to teach our model to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model!\n",
    "\n",
    "# instantiate the app with the fine-tuned model\n",
    "support_ticket_classifier = Support_Ticket_Classifier(\"mistral-7b\", instruction_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try running our fine-tune model against the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_support_ticket_classifier_finetuned = TruCustomApp(\n",
    "    support_ticket_classifier,\n",
    "    app_name=\"Support Ticket Classifier\",\n",
    "    app_version=\"mistral 7b - finetuned\",\n",
    "    feedbacks=[f_groundtruth]\n",
    ")\n",
    "\n",
    "for query in ground_truth['query']:\n",
    "    with tru_support_ticket_classifier_finetuned as recording:\n",
    "        label, rendered_prompt = support_ticket_classifier_finetuned.classify_ticket(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the leaderboard, our fine-tuned model performs well against our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move our model to production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cursor object\n",
    "cursor = snowflake_connection.cursor()\n",
    "\n",
    "# Define the SQL query to fetch the data\n",
    "query = \"SELECT * FROM SUPPORT_TICKET_CLASSIFICATION_DB.SUPPORT_TICKET_SCHEMA.SUPPORT_TICKETS_GROUND_TRUTH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load production data\n",
    "\n",
    "cursor.execute(query)\n",
    "ground_truth_production = cursor.fetch_pandas_all()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "\n",
    "ground_truth_production.rename(columns={'TICKET_ID': 'query_id', 'REQUEST': 'query', 'LABEL': 'expected_response'},\n",
    "                    inplace=True)\n",
    "\n",
    "# persist data in trulens database so we can fetch it from here in the future\n",
    "session.add_ground_truth_to_dataset(\n",
    "    dataset_name=\"support_ticket_eval_groundtruth_production\",\n",
    "    ground_truth_df=ground_truth_production,\n",
    "    dataset_metadata={\"split\": \"production\"},\n",
    ")\n",
    "\n",
    "ground_truth_production = session.get_ground_truth(\"support_ticket_eval_groundtruth_production\")\n",
    "\n",
    "ground_truth_production.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move app to production\n",
    "tru_support_ticket_classifier_finetuned = TruCustomApp(\n",
    "    support_ticket_classifier,\n",
    "    app_name=\"Support Ticket Classifier\",\n",
    "    app_version=\"mistral 7b - finetuned (production)\",\n",
    "    feedbacks=[f_groundtruth]\n",
    ")\n",
    "\n",
    "# run app on production data\n",
    "for query in ground_truth_production['query']:\n",
    "    with tru_support_ticket_classifier_finetuned as recording:\n",
    "        label, rendered_prompt = support_ticket_classifier_finetuned.classify_ticket(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In production, we've seen a drop in performance. We should examine the TruLens dashboard to learn more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we find when we look through the dashboard is that a new type of ticket has started to show up in production. This is commonly known as data drift.\n",
    "\n",
    "To combat this, we should further collect labels for this new data and fine-tune the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more fine-tuning\n",
    "\n",
    "# new app version with fine-tuned app\n",
    "\n",
    "# run on same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! We can now successfully label the support tickets describing issues with roaming fees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_leaderboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_trulens_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
