{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"analysis - bucketed_raw_staged_thinking_results.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate correlations for trajectory feedback metrics\n",
    "metrics = [\n",
    "    \"Step Relevance_score\",\n",
    "    \"Logical Consistency_score\",\n",
    "    \"Workflow Efficiency_score\",\n",
    "]\n",
    "targets = [\"LLM-Judge Rating\", \"GT\"]\n",
    "\n",
    "# Create correlation table\n",
    "correlation_data = []\n",
    "for metric in metrics:\n",
    "    correlations = []\n",
    "    for target in targets:\n",
    "        corr = df[metric].corr(df[target])\n",
    "        correlations.append(corr)\n",
    "    correlation_data.append(correlations)\n",
    "\n",
    "# Create DataFrame for better formatting\n",
    "correlation_table = pd.DataFrame(\n",
    "    data=correlation_data,\n",
    "    columns=[\"LLM Judge Rating\", \"Ground Truth\"],\n",
    "    index=[\"Step Relevance\", \"Logical Consistency\", \"Workflow Efficiency\"],\n",
    ")\n",
    "\n",
    "print(\"=== Trajectory Feedback Metrics Correlation Table ===\")\n",
    "print(correlation_table.round(3))\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\n=== Additional Analysis ===\")\n",
    "print(\n",
    "    f\"LLM Judge vs Ground Truth correlation: {df['LLM-Judge Rating'].corr(df['GT']):.3f}\"\n",
    ")\n",
    "\n",
    "# Check for any data issues\n",
    "print(\"\\n=== Data Quality Check ===\")\n",
    "for metric in metrics:\n",
    "    unique_values = df[metric].nunique()\n",
    "    print(f\"{metric}: {unique_values} unique values\")\n",
    "    if unique_values == 1:\n",
    "        print(f\"  Warning: {metric} has no variance (all values are the same)\")\n",
    "\n",
    "# Summary insights\n",
    "print(\"\\n=== Key Insights ===\")\n",
    "best_gt_predictor = correlation_table[\"Ground Truth\"].abs().idxmax()\n",
    "best_llm_predictor = correlation_table[\"LLM Judge Rating\"].abs().idxmax()\n",
    "print(f\"Best predictor of Ground Truth: {best_gt_predictor}\")\n",
    "print(f\"Best predictor of LLM Judge Rating: {best_llm_predictor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrices for each trajectory metric vs Ground Truth\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = [\n",
    "    \"Step Relevance_score\",\n",
    "    \"Logical Consistency_score\",\n",
    "    \"Workflow Efficiency_score\",\n",
    "]\n",
    "labels = [0, 1, 2]  # All possible classes\n",
    "label_names = [\"0\", \"1\", \"2\"]  # String labels for plotting\n",
    "\n",
    "print(\"=== Confusion Matrix Analysis (0=Poor, 1=Moderate, 2=Good) ===\\n\")\n",
    "\n",
    "# 1. LLM Judge vs Ground Truth\n",
    "cm_llm_gt = confusion_matrix(df[\"GT\"], df[\"LLM-Judge Rating\"], labels=labels)\n",
    "sns.heatmap(\n",
    "    cm_llm_gt,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"LLM Judge vs Ground Truth\")\n",
    "axes[0].set_xlabel(\"LLM Judge Rating\")\n",
    "axes[0].set_ylabel(\"Ground Truth\")\n",
    "\n",
    "print(\"LLM Judge vs Ground Truth:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        df[\"GT\"], df[\"LLM-Judge Rating\"], labels=labels, zero_division=\"warn\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2-4. Each trajectory metric vs Ground Truth\n",
    "for i, metric in enumerate(metrics, 1):\n",
    "    cm = confusion_matrix(df[\"GT\"], df[metric], labels=labels)\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=label_names,\n",
    "        yticklabels=label_names,\n",
    "        ax=axes[i],\n",
    "    )\n",
    "    axes[i].set_title(f'{metric.replace(\"_score\", \"\")} vs Ground Truth')\n",
    "    axes[i].set_xlabel(f'{metric.replace(\"_score\", \"\")}')\n",
    "    axes[i].set_ylabel(\"Ground Truth\")\n",
    "\n",
    "    print(f\"\\n{metric.replace('_score', '')} vs Ground Truth:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            df[\"GT\"], df[metric], labels=labels, zero_division=\"warn\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data distribution analysis\n",
    "print(\"\\n=== Data Distribution Analysis ===\")\n",
    "print(\"Ground Truth distribution:\")\n",
    "print(df[\"GT\"].value_counts().sort_index())\n",
    "print(\"\\nLLM Judge Rating distribution:\")\n",
    "print(df[\"LLM-Judge Rating\"].value_counts().sort_index())\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"\\n{metric} distribution:\")\n",
    "    print(df[metric].value_counts().sort_index())\n",
    "\n",
    "# Check for missing classes\n",
    "print(\"\\n=== Missing Classes Analysis ===\")\n",
    "all_columns = [\"GT\", \"LLM-Judge Rating\"] + metrics\n",
    "for col in all_columns:\n",
    "    unique_vals = set(df[col].unique())\n",
    "    missing_classes = set([0, 1, 2]) - unique_vals\n",
    "    if missing_classes:\n",
    "        print(f\"{col}: Missing classes {sorted(missing_classes)}\")\n",
    "    else:\n",
    "        print(f\"{col}: All classes present\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
