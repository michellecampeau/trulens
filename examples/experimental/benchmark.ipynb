{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "df = pd.read_csv(\"staged_thinking_data.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "sample_response = df.iloc[4][\"ResponseFull\"]\n",
    "\n",
    "\n",
    "# Try to parse it as JSON\n",
    "try:\n",
    "    parsed_try = json.loads(sample_response)\n",
    "    print(\"Parsed JSON structure:\")\n",
    "# print(json.dumps(parsed, indent=2))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Not valid JSON: {e}\")\n",
    "    print(\"Raw text content:\")\n",
    "    # print(sample_response[:200])\n",
    "\n",
    "print(len(parsed_try[\"content\"]))\n",
    "print(parsed_try[\"content\"][6:8])\n",
    "formatted_response = json.dumps(parsed_try[\"content\"])\n",
    "type(formatted_response)\n",
    "\n",
    "\n",
    "def format_execution_trace(content: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format the content list as a readable execution trace for trajectory evaluation.\n",
    "\n",
    "    Args:\n",
    "        content: List of execution steps from the ResponseFull JSON\n",
    "\n",
    "    Returns:\n",
    "        Formatted string representation of the execution trace\n",
    "    \"\"\"\n",
    "    if not isinstance(content, list):\n",
    "        return str(content)\n",
    "\n",
    "    # Option 1: Clean JSON formatting\n",
    "    formatted_trace = json.dumps(content, indent=2, ensure_ascii=False)\n",
    "    formatted_trace = formatted_trace.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    # Option 2: More readable step-by-step format (alternative)\n",
    "    # formatted_trace = format_as_readable_steps(content)\n",
    "\n",
    "    return formatted_trace\n",
    "\n",
    "\n",
    "def format_as_readable_steps(content: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Alternative: Format as more readable step-by-step execution trace.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    for i, step in enumerate(content, 1):\n",
    "        step_type = step.get(\"type\", \"unknown\")\n",
    "\n",
    "        if step_type == \"thinking\":\n",
    "            thinking_text = step.get(\"thinking\", {}).get(\"text\", \"\")\n",
    "            steps.append(f\"thinking: {thinking_text}\")\n",
    "\n",
    "        elif step_type == \"tool_use\":\n",
    "            tool_info = step.get(\"tool_use\", {})\n",
    "            tool_name = tool_info.get(\"name\", \"unknown\")\n",
    "            tool_type = tool_info.get(\"type\", \"unknown\")\n",
    "            tool_input = tool_info.get(\"input\", {})\n",
    "            steps.append(f\"tool_name: {tool_name}\")\n",
    "            steps.append(f\"tool_type: {tool_type}\")\n",
    "            steps.append(f\"input: {json.dumps(tool_input, indent=2)}\")\n",
    "\n",
    "        elif step_type == \"tool_results\":\n",
    "            tool_results = step.get(\"tool_results\", {})\n",
    "            steps.append(f\"tool_results: {json.dumps(tool_results, indent=2)}\")\n",
    "\n",
    "        elif step_type == \"text\":\n",
    "            text_content = step.get(\"text\", \"\")\n",
    "            steps.append(f\"text: {text_content}\")\n",
    "\n",
    "        steps.append(\"\")  # Empty line between steps\n",
    "\n",
    "    return \"\\n\".join(steps)\n",
    "\n",
    "\n",
    "# readable_trace = format_as_readable_steps(parsed_try['content'])\n",
    "# print(f\"readable_trace: {readable_trace}\")\n",
    "formatted_trace = format_execution_trace(parsed_try[\"content\"])\n",
    "print(f\"raw_trace: {formatted_trace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create snowpark session.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# snowflake_connection_parameters = {\n",
    "#     \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "#     \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "#     \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "#     \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "#     \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "#     \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "#     \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "# }\n",
    "\n",
    "\n",
    "snowflake_connection_parameters = {\n",
    "    \"account\": \"SNOWHOUSE\",\n",
    "    \"user\": \"ajia\",\n",
    "    \"authenticator\": \"externalbrowser\",\n",
    "}\n",
    "print(snowflake_connection_parameters)\n",
    "snowpark_session = Session.builder.configs(\n",
    "    snowflake_connection_parameters\n",
    ").create()\n",
    "\n",
    "# TruSession is no longer required as long as snowflake connector exists\n",
    "# sf_connector = SnowflakeConnector(snowpark_session=snowpark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.cortex import Cortex\n",
    "\n",
    "provider = Cortex(\n",
    "    model_engine=\"claude-3-7-sonnet\", snowpark_session=snowpark_session\n",
    ")\n",
    "# provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "\n",
    "# Create feedback functions without selectors (better for direct string evaluation)\n",
    "# These can be called directly on strings, which is what you want for your DataFrame processing\n",
    "\n",
    "feedback_functions = {\n",
    "    \"Step Relevance\": provider.trajectory_step_relevance_with_cot_reasons,\n",
    "    \"Logical Consistency\": provider.trajectory_logical_consistency_with_cot_reasons,\n",
    "    \"Workflow Efficiency\": provider.trajectory_workflow_efficiency_with_cot_reasons,\n",
    "    \"Plan Adherence\": provider.trajectory_plan_adherence_with_cot_reasons,\n",
    "}\n",
    "\n",
    "for name in feedback_functions:\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIL: GAIA Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "gaia_file = json.load(open(\"0242ca2533fac5b8b604a9060b3e15d6.json\"))\n",
    "gaia_log = gaia_file[\"spans\"][0]\n",
    "\n",
    "print(gaia_log[\"child_spans\"][1].keys())\n",
    "gaia_trace = format_execution_trace(gaia_log[\"child_spans\"][1][\"child_spans\"])\n",
    "print(gaia_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.trajectory_step_relevance_with_cot_reasons(gaia_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.trajectory_logical_consistency_with_cot_reasons(gaia_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.trajectory_workflow_efficiency_with_cot_reasons(gaia_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.trajectory_plan_adherence_with_cot_reasons(gaia_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tyler V2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def evaluate_row_content(\n",
    "    row: pd.Series, feedback_functions: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a single row's content with all feedback functions.\n",
    "\n",
    "    Args:\n",
    "        row: A pandas Series representing a row from the DataFrame\n",
    "        feedback_functions: List of feedback functions to apply\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with feedback results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"conversation_id\": row[\"conversation_id\"],\n",
    "        \"question\": row[\"Question\"],\n",
    "        \"answer\": row[\"Answer\"],\n",
    "        \"status\": row[\"Status\"],\n",
    "        \"expected_answer\": row[\"ExpectedAnswer\"],\n",
    "        \"LLM-judge analysis\": row[\"Analysis\"],\n",
    "        \"LLM-judge reasoning\": row[\"Reasoning\"],\n",
    "        \"LLM-judge rating\": row[\"Rating\"],\n",
    "        \"ResponseFull\": row[\"ResponseFull\"],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Parse the JSON from ResponseFull\n",
    "        parsed = json.loads(\n",
    "            row[\"ResponseFull\"]\n",
    "        )  # Cast to string to fix type issue\n",
    "\n",
    "        # Extract the content\n",
    "        content = parsed.get(\"content\", [])\n",
    "\n",
    "        # Format as execution trace using the readable format\n",
    "        if isinstance(content, list):\n",
    "            content_str = format_execution_trace(content)\n",
    "        else:\n",
    "            content_str = str(content)\n",
    "\n",
    "        # Apply each feedback function\n",
    "        for feedback_name, feedback_func in feedback_functions.items():\n",
    "            try:\n",
    "                # Call the feedback function directly with the formatted trace string\n",
    "                result = feedback_func(content_str)\n",
    "\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    score, metadata = result\n",
    "                    results[f\"{feedback_name}_score\"] = score\n",
    "                    results[f\"{feedback_name}_reasons\"] = metadata.get(\n",
    "                        \"reason\", \"\"\n",
    "                    )\n",
    "                    # Convert metadata to string for CSV storage\n",
    "                else:\n",
    "                    results[f\"{feedback_name}_score\"] = result\n",
    "                    results[f\"{feedback_name}_reasons\"] = \"\"\n",
    "                    results[f\"{feedback_name}_metadata\"] = \"{}\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error evaluating {feedback_name} on {row['conversation_id']}: {e}\"\n",
    "                )\n",
    "                results[f\"{feedback_name}_score\"] = float(\"nan\")\n",
    "                results[f\"{feedback_name}_reasons\"] = f\"Error: {str(e)}\"\n",
    "                results[f\"{feedback_name}_metadata\"] = (\n",
    "                    f'{{\"error\": \"{str(e)}\"}}'\n",
    "                )\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse JSON for {row['conversation_id']}: {e}\")\n",
    "        # Set all feedback results to NaN for this row\n",
    "        for feedback_name in feedback_functions:\n",
    "            results[f\"{feedback_name}_score\"] = float(\"nan\")\n",
    "            results[f\"{feedback_name}_reasons\"] = f\"JSON Parse Error: {str(e)}\"\n",
    "            results[f\"{feedback_name}_metadata\"] = (\n",
    "                f'{{\"json_error\": \"{str(e)}\"}}'\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Process all rows and collect results\n",
    "print(\"Processing rows for feedback evaluation...\")\n",
    "all_results = []\n",
    "\n",
    "for i, (idx, row) in enumerate(df.iterrows()):\n",
    "    print(f\"Processing row {i + 1}/{len(df)}: {row['conversation_id']}\")\n",
    "\n",
    "    # Evaluate this row using the direct feedback functions\n",
    "    row_results = evaluate_row_content(row, feedback_functions)\n",
    "    all_results.append(row_results)\n",
    "\n",
    "# Convert to DataFrame - this creates one row per conversation_id\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display basic information about results\n",
    "print(f\"\\nProcessing complete! Evaluated {len(results_df)} rows.\")\n",
    "print(f\"Results DataFrame shape: {results_df.shape}\")\n",
    "print(f\"Columns: {list(results_df.columns)}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 3 rows of results:\")\n",
    "print(results_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.trajectory_step_relevance_with_cot_reasons(\n",
    "    format_execution_trace(parsed_try[\"content\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.trajectory_logical_consistency_with_cot_reasons(\n",
    "    format_execution_trace(parsed_try[\"content\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df[\"Step Relevance_reasons\"].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show statistics for each feedback function\n",
    "print(\"\\nFeedback Score Statistics:\")\n",
    "for feedback_name in feedback_functions:\n",
    "    score_col = f\"{feedback_name}_score\"\n",
    "    if score_col in results_df.columns:\n",
    "        valid_scores = results_df[score_col].dropna()\n",
    "        if len(valid_scores) > 0:\n",
    "            print(f\"\\n{feedback_name}:\")\n",
    "            print(f\"  Mean: {valid_scores.mean():.3f}\")\n",
    "            print(f\"  Std:  {valid_scores.std():.3f}\")\n",
    "            print(f\"  Min:  {valid_scores.min():.3f}\")\n",
    "            print(f\"  Max:  {valid_scores.max():.3f}\")\n",
    "        else:\n",
    "            print(f\"\\n{feedback_name}: No valid scores\")\n",
    "\n",
    "# Save results to CSV\n",
    "output_filename = \"claude37sonnet_staged_thinking_feedback_raw_results_3.csv\"\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nResults saved to '{output_filename}'\")\n",
    "\n",
    "# Also save a summary statistics CSV\n",
    "summary_stats = []\n",
    "for feedback_name in feedback_functions:\n",
    "    score_col = f\"{feedback_name}_score\"\n",
    "    if score_col in results_df.columns:\n",
    "        valid_scores = results_df[score_col].dropna()\n",
    "        if len(valid_scores) > 0:\n",
    "            summary_stats.append({\n",
    "                \"feedback_function\": feedback_name,\n",
    "                \"mean_score\": valid_scores.mean(),\n",
    "                \"std_score\": valid_scores.std(),\n",
    "                \"min_score\": valid_scores.min(),\n",
    "                \"max_score\": valid_scores.max(),\n",
    "            })\n",
    "\n",
    "if summary_stats:\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_filename = (\n",
    "        \"claude37sonnet_staged_thinking_feedback_raw_summary_stats_3.csv\"\n",
    "    )\n",
    "    summary_df.to_csv(summary_filename, index=False)\n",
    "    print(f\"Summary statistics saved to '{summary_filename}'\")\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(summary_df)\n",
    "\n",
    "# Optional: Display some example reasons for debugging\n",
    "print(\"\\nExample Feedback Reasons:\")\n",
    "for feedback_name in feedback_functions:\n",
    "    reasons_col = f\"{feedback_name}_reasons\"\n",
    "    if reasons_col in results_df.columns:\n",
    "        # Get first non-empty reason\n",
    "        non_empty_reasons = results_df[\n",
    "            results_df[reasons_col].notna() & (results_df[reasons_col] != \"\")\n",
    "        ][reasons_col]\n",
    "        if len(non_empty_reasons) > 0:\n",
    "            print(f\"\\n{feedback_name} - Example reason (first 200 chars):\")\n",
    "            first_reason = str(list(non_empty_reasons)[0])\n",
    "            print(f\"  {first_reason[:200]}...\")\n",
    "        else:\n",
    "            print(f\"\\n{feedback_name}: No reasons available\")\n",
    "\n",
    "print(\"\\nFiles created:\")\n",
    "print(\n",
    "    f\"- {output_filename}: Full results with all feedback scores and metadata\"\n",
    ")\n",
    "print(f\"- {summary_filename}: Summary statistics for each feedback function\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
