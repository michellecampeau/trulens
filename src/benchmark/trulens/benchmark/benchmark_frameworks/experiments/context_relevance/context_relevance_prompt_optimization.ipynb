{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_trec_dl_passage_benchmark,\n",
    ")\n",
    "\n",
    "trec_2021_samples = list(\n",
    "    generate_trec_dl_passage_benchmark(\n",
    "        max_samples_per_query_per_score=4,\n",
    "        dataset_path=\"msmarco-passage-v2/trec-dl-2021/judged\",\n",
    "    )\n",
    ")\n",
    "trec_2022_samples = list(\n",
    "    generate_trec_dl_passage_benchmark(\n",
    "        max_samples_per_query_per_score=4,\n",
    "        dataset_path=\"msmarco-passage-v2/trec-dl-2022/judged\",\n",
    "    )\n",
    ")\n",
    "trec_combined = trec_2021_samples + trec_2022_samples\n",
    "\n",
    "trec_combined_df = pd.DataFrame(trec_combined)\n",
    "\n",
    "print(f\"Totoal number of samples: {len(trec_combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    visualize_expected_score_distribution,\n",
    ")\n",
    "\n",
    "trec_combined_relevance_scores = [\n",
    "    entry[\"expected_score\"] for _, entry in trec_combined_df.iterrows()\n",
    "]\n",
    "visualize_expected_score_distribution(trec_combined_relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def balanced_split(\n",
    "    df,\n",
    "    label_column=\"expected_score\",\n",
    "    train_size=0.6,\n",
    "    dev_size=0.2,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into train, dev, and test sets with balanced labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        label_column (str): The column containing the labels to balance on.\n",
    "        train_size (float): Proportion of the data to use for training.\n",
    "        dev_size (float): Proportion of the data to use for dev/validation.\n",
    "        test_size (float): Proportion of the data to use for testing.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        train_df (pd.DataFrame): Training split.\n",
    "        dev_df (pd.DataFrame): Development/validation split.\n",
    "        test_df (pd.DataFrame): Testing split.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        abs(train_size + dev_size + test_size - 1.0) < 1e-5\n",
    "    ), \"Sizes must sum to 1.0\"\n",
    "\n",
    "    # Step 1: Split train+dev and test\n",
    "    train_dev_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        stratify=df[label_column],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Step 2: Calculate relative size for dev split from train+dev\n",
    "    dev_relative_size = dev_size / (train_size + dev_size)\n",
    "\n",
    "    # Step 3: Split train and dev\n",
    "    train_df, dev_df = train_test_split(\n",
    "        train_dev_df,\n",
    "        test_size=dev_relative_size,\n",
    "        stratify=train_dev_df[label_column],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return train_df, dev_df, test_df\n",
    "\n",
    "\n",
    "train_df, dev_df, test_df = balanced_split(\n",
    "    trec_combined_df, train_size=0.6, dev_size=0.2, test_size=0.2\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Dev size: {len(dev_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [entry[\"expected_score\"] for _, entry in train_df.iterrows()]\n",
    "visualize_expected_score_distribution(train_scores)\n",
    "\n",
    "dev_scores = [entry[\"expected_score\"] for _, entry in dev_df.iterrows()]\n",
    "visualize_expected_score_distribution(dev_scores)\n",
    "\n",
    "test_scores = [entry[\"expected_score\"] for _, entry in test_df.iterrows()]\n",
    "visualize_expected_score_distribution(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement TruLens' `context_relevance_with_cot_reasons` in AdalFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ContextRelevance.system_prompt)\n",
    "\n",
    "cortex_search_custom_prompt = \"\"\"You are an expert search result rater. You are given a user query and a search result. Your task is to rate the search result based on its relevance to the user query. You should rate the search result on a scale of 0 to 3, where:\n",
    "    0: The search result has no relevance to the user query.\n",
    "    1: The search result has low relevance to the user query. In this case the search result may contain some information which seems very slightly related to the user query but not enough information to answer the user query. The search result contains some references or very limited information about some entities present in the user query. In case the query is a statement on a topic, the search result should be tangentially related to it.\n",
    "    2: The search result has medium relevance to the user query. If the user query is a question, the search result may contain some information that is relevant to the user query but not enough information to answer the user query. If the user query is a search phrase/sentence, either the search result is centered around about most but not all entities present in the user query, or if all the entities are present in the result, the search result while not being centered around it has medium level of relevance. In case the query is a statement on a topic, the search result should be related to the topic.\n",
    "    3: The search result has high relevance to the user query. If the user query is a question, the search result contains information that can answer the user query. Otherwise if the search query is a search phrase/sentence, it provides relevant information about all entities that are present in the user query and the search result is centered around the entities mentioned in the query. In case the query is a statement on a topic, the search result should be either be directly addressing it or be on the same topic.\n",
    "    \n",
    "    You should think step by step about the user query and the search result and rate the search result. You should also provide a reasoning for your rating.\n",
    "    \n",
    "    Use the following format:\n",
    "    Rating: Example Rating\n",
    "    Reasoning: Example Reasoning\n",
    "    \n",
    "    Now given the user query and search result below, rate the search result based on its relevance to the user query and provide a reasoning for your rating.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORTEX_FEW_SHOT_DEMOS = \"\"\"\n",
    "### Examples\n",
    "    Example:\n",
    "    Example 1:\n",
    "    INPUT:\n",
    "    User Query: What is the definition of an accordion?\n",
    "    Search Result: Accordion definition, Also called piano accordion. a portable wind instrument having a large bellows for forcing air through small metal reeds, a keyboard for the right hand, and buttons for sounding single bass notes or chords for the left hand. a similar instrument having single-note buttons instead of a keyboard.\n",
    "    OUTPUT:\n",
    "    Rating: 3\n",
    "    Reasoning: In this case the search query is a question. The search result directly answers the user question for the definition of an accordion, hence it has high relevance to the user query.\n",
    "    \n",
    "    Example 2:\n",
    "    INPUT:\n",
    "    User Query: dark horse\n",
    "    Search Result: Darkhorse is a person who everyone expects to be last in a race. Think of it this way. The person who looks like he can never get laid defies the odds and gets any girl he can by being sly,shy and cunning. Although he\\'s not a player, he can really charm the ladies.\n",
    "    OUTPUT:\n",
    "    Rating: 3\n",
    "    Reasoning: In this case the search query is a search phrase mentioning \\'dark horse\\'. The search result contains information about the term \\'dark horse\\' and provides a definition for it and is centered around it. Hence it has high relevance to the user query.\n",
    "    \n",
    "    Example 3:\n",
    "    INPUT:\n",
    "    User Query: Global warming and polar bears\n",
    "    Search Result: Polar bear The polar bear is a carnivorous bear whose native range lies largely within the Arctic Circle, encompassing the Arctic Ocean, its surrounding seas and surrounding land masses. It is a large bear, approximately the same size as the omnivorous Kodiak bear (Ursus arctos middendorffi).\n",
    "    OUTPUT:\n",
    "    Rating: 2\n",
    "    Reasoning: In this case the search query is a search phrase mentioning two entities \\'Global warming\\' and \\'polar bears\\'. The search result contains is centered around the polar bear which is one of the two entities in the search query. Therefore it addresses most of the entities present and hence has medium relevance. \n",
    "    \n",
    "    Example 4:\n",
    "    INPUT:\n",
    "    User Query: Snowflake synapse private link\n",
    "    Search Result: \"This site can\\'t be reached\" error when connecting to Snowflake via Private Connectivity\\nThis KB article addresses an issue that prevents connections to Snowflake failing with: \"This site can\\'t be reached\" ISSUE: Attempting to reach Snowflake via Private Connectivity fails with the \"This site can\\'t be reached\" error\n",
    "    OUTPUT:\n",
    "    Rating: 1\n",
    "    Reasoning: In this case the search result is a search query mentioning \\'Snowflake synapse private link\\'. However the search result doesn\\'t contain information about it. However it shows an error message for a generic private link which is tangentially related to the query, since snowflake synapse private link is a type of private link. Hence it has low relevance to the user query.\n",
    "    \n",
    "    Example 5:\n",
    "    INPUT:\n",
    "    User Query: The Punisher is American.\n",
    "    Search Result: The Rev(Samuel Smith) is a fictional character, a supervillain appearing in American comic books published by Marvel Comics. Created by Mike Baron and Klaus Janson, the character made his first appearance in The Punisher Vol. 2, #4 (November 1987). He is an enemy of the Punisher.\n",
    "    OUTPUT:\n",
    "    Rating: 1\n",
    "    Reasoning: In this case the search query is a statement concerning the Punisher. However the search result is about a character called Rev, who is an enemy of the Punisher. The search result is tangentially related to the user query but does not address topic about Punisher being an American. Hence it has low relevance to the user query.\n",
    "\n",
    "    Example 6:\n",
    "    INPUT:\n",
    "    User Query: query_history\n",
    "    Search Result: The function task_history() is not enough for the purposes when the required result set is more than 10k.If we perform UNION between information_schema and account_usage , then we will get more than 10k records along with recent records as from information_schema.query_history to snowflake.account_usage.query_history is 45 mins behind.\n",
    "    OUTPUT:\n",
    "    Rating: 1\n",
    "    Reasoning: In this case the search query mentioning one entity \\'query_history\\'. The search result is neither centered around it and neither has medium relevance, it only contains an unimportant reference to it. Hence it has low relevance to the user query.\n",
    "    \n",
    "    Example 7:\n",
    "    INPUT:\n",
    "    User Query: Who directed pulp fiction?\n",
    "    Search Result: Life on Earth first appeared as early as 4.28 billion years ago, soon after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago.\n",
    "    OUTPUT:\n",
    "    Rating: 0\n",
    "    Reasoning: In the case the search query is a question. However the search result does is completely unrelated to it. Hence the search result is completely irrelevant to the movie pulp fiction. \n",
    "    ###\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import adalflow as adal\n",
    "from adalflow.optim.types import ParameterType\n",
    "from trulens.feedback import generated as feedback_generated\n",
    "\n",
    "few_shot_template = r\"\"\"<START_OF_SYSTEM_PROMPT>\n",
    "{{system_prompt}}\n",
    "{# Few shot demos #}\n",
    "{% if few_shot_demos is not none %}\n",
    "Here are some examples:\n",
    "{{few_shot_demos}}\n",
    "{% endif %}\n",
    "<END_OF_SYSTEM_PROMPT>\n",
    "<START_OF_USER>\n",
    "{{user_prompt}}\n",
    "<END_OF_USER>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ContextRelevanceTaskPipeline(adal.Component):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_prompt: str,\n",
    "        model_client: adal.ModelClient,\n",
    "        model_kwargs: Dict,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        system_prompt = adal.Parameter(\n",
    "            data=target_prompt,\n",
    "            role_desc=\"To give task instruction to the language model in the system prompt\",\n",
    "            requires_opt=True,\n",
    "            param_type=ParameterType.PROMPT,\n",
    "            instruction_to_optimizer=\"You can try to show examples to see if it helps. Make sure the model is being very critical and strict with its ratings, avoid being lenient / false positves to get better results.\",\n",
    "        )\n",
    "        few_shot_demos = adal.Parameter(\n",
    "            data=CORTEX_FEW_SHOT_DEMOS,\n",
    "            role_desc=\"To provide few shot demos to the language model\",\n",
    "            requires_opt=False,  # Changed to True for few-shot learning\n",
    "            param_type=ParameterType.DEMOS,\n",
    "        )\n",
    "\n",
    "        self.evaluate_relevance = adal.Generator(\n",
    "            model_client=model_client,\n",
    "            model_kwargs=model_kwargs,\n",
    "            template=few_shot_template,\n",
    "            prompt_kwargs={\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"few_shot_demos\": few_shot_demos,\n",
    "            },\n",
    "            use_cache=True,\n",
    "            output_processors=self.parse_output,\n",
    "        )\n",
    "\n",
    "    @adal.fun_to_component\n",
    "    def parse_output(response: str):\n",
    "        # Extract the rating and reasoning from the output\n",
    "        response = response.strip()\n",
    "\n",
    "        rating = None\n",
    "        reasoning = None\n",
    "        try:\n",
    "            for line in response.split(\"\\n\"):\n",
    "                if line.startswith(\"Rating:\"):\n",
    "                    rating = int(line.split(\":\")[1].strip())\n",
    "                elif line.startswith(\"Reasoning:\"):\n",
    "                    reasoning = line.split(\":\")[1].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "\n",
    "        return rating, reasoning\n",
    "\n",
    "    @adal.fun_to_component\n",
    "    def parse_trulens_output(response: str) -> Tuple[float, Dict]:\n",
    "        score, reason = None, None\n",
    "        if response and \"Supporting Evidence\" in response:\n",
    "            score = -1\n",
    "            supporting_evidence = None\n",
    "            criteria = None\n",
    "            for line in response.split(\"\\n\"):\n",
    "                if \"Score\" in line:\n",
    "                    score = (\n",
    "                        feedback_generated.re_configured_rating(\n",
    "                            line,\n",
    "                            min_score_val=0,\n",
    "                            max_score_val=3,\n",
    "                        )\n",
    "                    ) / 3\n",
    "                criteria_lines = []\n",
    "                supporting_evidence_lines = []\n",
    "                collecting_criteria = False\n",
    "                collecting_evidence = False\n",
    "\n",
    "                for line in response.split(\"\\n\"):\n",
    "                    if \"Criteria:\" in line:\n",
    "                        criteria_lines.append(\n",
    "                            line.split(\"Criteria:\", 1)[1].strip()\n",
    "                        )\n",
    "                        collecting_criteria = True\n",
    "                        collecting_evidence = False\n",
    "                    elif \"Supporting Evidence:\" in line:\n",
    "                        supporting_evidence_lines.append(\n",
    "                            line.split(\"Supporting Evidence:\", 1)[1].strip()\n",
    "                        )\n",
    "                        collecting_evidence = True\n",
    "                        collecting_criteria = False\n",
    "                    elif collecting_criteria:\n",
    "                        if \"Supporting Evidence:\" not in line:\n",
    "                            criteria_lines.append(line.strip())\n",
    "                        else:\n",
    "                            collecting_criteria = False\n",
    "                    elif collecting_evidence:\n",
    "                        if \"Criteria:\" not in line:\n",
    "                            supporting_evidence_lines.append(line.strip())\n",
    "                        else:\n",
    "                            collecting_evidence = False\n",
    "\n",
    "                criteria = \"\\n\".join(criteria_lines).strip()\n",
    "                supporting_evidence = \"\\n\".join(\n",
    "                    supporting_evidence_lines\n",
    "                ).strip()\n",
    "            reason = {\n",
    "                \"reason\": (\n",
    "                    f\"{'Criteria: ' + str(criteria)}\\n\"\n",
    "                    f\"{'Supporting Evidence: ' + str(supporting_evidence)}\"\n",
    "                )\n",
    "            }\n",
    "            score = score\n",
    "            reason = reason\n",
    "\n",
    "        else:\n",
    "            if not response:\n",
    "                score = 0\n",
    "                reason = {\"reason\": \"No response generated.\"}\n",
    "            else:\n",
    "                score = (\n",
    "                    feedback_generated.re_configured_rating(\n",
    "                        response,\n",
    "                        min_score_val=0,\n",
    "                        max_score_val=3,\n",
    "                    )\n",
    "                ) / 3\n",
    "                warnings.warn(\n",
    "                    \"No supporting evidence provided. Returning score only.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                score = score\n",
    "                reason = {}\n",
    "\n",
    "        score_pattern = re.compile(r\"Score:\\s*([0-9.]+)\")\n",
    "        match = score_pattern.search(reason.get(\"reason\", \"\"))\n",
    "        normalized_reason = None\n",
    "        if match:\n",
    "            original_reason_score = float(match.group(1))\n",
    "            normalized_reason_score = (original_reason_score) / 3\n",
    "\n",
    "            # Ensure the formatting matches exactly\n",
    "            original_string = f\"Score: {int(original_reason_score)}\"\n",
    "            replacement_string = f\"Score: {normalized_reason_score}\"\n",
    "            normalized_reason = reason.copy()\n",
    "            normalized_reason[\"reason\"] = normalized_reason[\"reason\"].replace(\n",
    "                original_string, replacement_string\n",
    "            )\n",
    "\n",
    "        if normalized_reason is not None:\n",
    "            return score, normalized_reason\n",
    "        else:\n",
    "            return score, reason\n",
    "\n",
    "    def call(\n",
    "        self, query: str, context: str, id: Optional[str] = None\n",
    "    ) -> Union[adal.GeneratorOutput, adal.Parameter]:\n",
    "        user_prompt = \"\"\"INPUT:\n",
    "                        User Query: {query}\n",
    "                        Search Result: {context}\n",
    "                        OUTPUT:\\n\"\"\".format(query=query, context=context)\n",
    "\n",
    "        return self.evaluate_relevance(\n",
    "            prompt_kwargs={\"user_prompt\": user_prompt}, id=id\n",
    "        )\n",
    "\n",
    "    # def call(\n",
    "    #     self,\n",
    "    #     query: str,\n",
    "    #     context: str,\n",
    "    #     id: Optional[str] = None,\n",
    "    # ) -> Union[adal.GeneratorOutput, adal.Parameter]:\n",
    "\n",
    "    #     user_prompt = \"\"\"QUESTION: {query}\n",
    "    #     CONTEXT: {context}\n",
    "\n",
    "    #     RELEVANCE:\n",
    "    #     \"\"\".format(query=query, context=context).replace(\n",
    "    #         \"RELEVANCE:\", feedback_prompts.COT_REASONS_TEMPLATE\n",
    "    #     )\n",
    "\n",
    "    #     return self.evaluate_relevance(\n",
    "    #         prompt_kwargs={\"user_prompt\": user_prompt}, id=id\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from adalflow.components.model_client.openai_client import AzureOpenAIClient\n",
    "\n",
    "az_gpt_4o_model = {\n",
    "    \"model_client\": AzureOpenAIClient(),\n",
    "    \"model_kwargs\": {\n",
    "        \"model\": os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.99,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"stop\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "# gpt_4o_model = {\n",
    "#     \"model_client\": OpenAIClient(),\n",
    "#     \"model_kwargs\": {\n",
    "#         \"model\": \"gpt-4o\",\n",
    "#         \"max_tokens\": 4000,\n",
    "#         \"temperature\": 0.0,\n",
    "#         \"top_p\": 0.99,\n",
    "#         \"frequency_penalty\": 0,\n",
    "#         \"presence_penalty\": 0,\n",
    "#         \"stop\": None,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "task_pipeline = ContextRelevanceTaskPipeline(\n",
    "    target_prompt=cortex_search_custom_prompt, **az_gpt_4o_model\n",
    ")\n",
    "print(task_pipeline)\n",
    "\n",
    "output = task_pipeline(\n",
    "    query=\"Is apple safe to eat?\", context=\"All fruits are edible\"\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_pipeline.train()  # set to train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_pipeline.train()  # set to train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "import uuid\n",
    "\n",
    "from adalflow.datasets.types import Example\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrecDLData(Example):\n",
    "    __doc__ = \"\"\"A dataclass for representing examples in the TREC DL (passage retrieval) dataset.\"\"\"\n",
    "\n",
    "    id: str = field(\n",
    "        metadata={\"desc\": \"The unique identifier of the example\", \"type\": \"id\"},\n",
    "        default_factory=lambda: str(\n",
    "            uuid.uuid4()\n",
    "        ),  # Ensures a unique UUID for each instance\n",
    "    )\n",
    "    query: Optional[str] = field(\n",
    "        metadata={\"desc\": \"The query from user.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    expected_response: Optional[str] = field(\n",
    "        metadata={\"desc\": \"The retrieved context for the query.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    expected_score: Optional[float] = field(\n",
    "        metadata={\"desc\": \"The expected relevance score for the answer.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = [\n",
    "    TrecDLData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"] * 3,\n",
    "    )\n",
    "    for _, row in train_df.iterrows()\n",
    "]\n",
    "val_dataset = [\n",
    "    TrecDLData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"] * 3,\n",
    "    )\n",
    "    for _, row in dev_df.iterrows()\n",
    "]\n",
    "test_dataset = [\n",
    "    TrecDLData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"] * 3,\n",
    "    )\n",
    "    for _, row in test_df.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "def context_relevance_eval_fn(y: float, y_gt: float) -> float:\n",
    "    return 1.0 if y == y_gt else 0.0\n",
    "\n",
    "\n",
    "def weighted_relevance_loss(\n",
    "    y: float, y_gt: float, false_positive_weight\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Penalizes false positives more heavily and keeps the loss in [0, 1].\n",
    "    \"\"\"\n",
    "    # Identify the type of error\n",
    "    if y > y_gt:  # False positive\n",
    "        penalty = false_positive_weight\n",
    "    elif y != y_gt:  # Other mismatches (false negatives)\n",
    "        penalty = 1.0\n",
    "    else:  # Correct predictions\n",
    "        return 0.0\n",
    "\n",
    "    # Normalize the penalty to keep the loss in [0, 1]\n",
    "    normalized_loss = penalty / (false_positive_weight + 1.0)\n",
    "\n",
    "    return (\n",
    "        1 - normalized_loss\n",
    "    )  # textual loss higher the better (UNLIKE typical ML loss)\n",
    "\n",
    "\n",
    "class ContextRelevanceAdalComponentOnTrecDL(adal.AdalComponent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_prompt: str,\n",
    "        model_client: adal.ModelClient,\n",
    "        model_kwargs: Dict,\n",
    "        backward_engine_model_config: Dict = None,\n",
    "        teacher_model_config: Dict = None,\n",
    "        text_optimizer_model_config: Dict = None,\n",
    "    ):\n",
    "        task = ContextRelevanceTaskPipeline(\n",
    "            target_prompt, model_client, model_kwargs\n",
    "        )\n",
    "        # eval_fn = AnswerMatchAcc(type=\"exact_match\").compute_single_item\n",
    "        eval_fn = context_relevance_eval_fn\n",
    "        loss_fn = adal.EvalFnToTextLoss(\n",
    "            eval_fn=lambda y, y_gt: weighted_relevance_loss(\n",
    "                y, y_gt, false_positive_weight=3.0\n",
    "            ),\n",
    "            eval_fn_desc=\"Give a lower score when the model gives higher rating than ground truth to avoid being too lenient (y higher than y_gt)\",\n",
    "        )\n",
    "\n",
    "        # eval_fn = context_relevance_eval_fn\n",
    "        # loss_fn = adal.EvalFnToTextLoss(\n",
    "        #     eval_fn=eval_fn,\n",
    "        #     eval_fn_desc=\"\"\"Binarized / label unification  - y and y_gt are both in [0, 1], y_binary = 1 if y >= 0.5 else 0; y_gt_binary = 1 if y_gt >= 0.5 else 0;\n",
    "        #     so 1 if y_binary == y_gt_binary else 0 (after label unification)\"\"\",\n",
    "        # )\n",
    "\n",
    "        super().__init__(task=task, eval_fn=eval_fn, loss_fn=loss_fn)\n",
    "        self.backward_engine_model_config = backward_engine_model_config\n",
    "        self.teacher_model_config = teacher_model_config\n",
    "        self.text_optimizer_model_config = text_optimizer_model_config\n",
    "\n",
    "    def prepare_task(self, sample: TrecDLData):\n",
    "        return self.task.call, {\n",
    "            \"query\": sample.query,\n",
    "            \"context\": sample.expected_response,\n",
    "            \"id\": sample.id,\n",
    "        }\n",
    "\n",
    "    def prepare_loss(self, sample: TrecDLData, pred: adal.Parameter):\n",
    "        # prepare the gt and pred for the loss function\n",
    "        y_gt = adal.Parameter(\n",
    "            name=\"y_gt\",\n",
    "            data=sample.expected_score,\n",
    "            eval_input=sample.expected_score,\n",
    "            requires_opt=False,\n",
    "        )\n",
    "\n",
    "        # print(f\"pred: {pred}\")\n",
    "        # print(f\"pred.full_response: {pred.full_response}\")\n",
    "\n",
    "        pred.eval_input = (\n",
    "            pred.full_response.data[0]\n",
    "            if pred and pred.full_response and len(pred.full_response.data) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        return self.loss_fn, {\"kwargs\": {\"y\": pred, \"y_gt\": y_gt}}\n",
    "\n",
    "    def prepare_eval(self, sample: TrecDLData, y_pred: adal.GeneratorOutput):\n",
    "        y_label = -1\n",
    "        if (\n",
    "            y_pred\n",
    "            and y_pred.data\n",
    "            and len(y_pred.data) > 0\n",
    "            and isinstance(y_pred.data[0], (int, float))\n",
    "        ):\n",
    "            y_label = y_pred.data[0]\n",
    "\n",
    "        # print(y_pred, y_label, sample.expected_score)\n",
    "        return self.eval_fn, {\"y\": y_label, \"y_gt\": sample.expected_score}\n",
    "\n",
    "    def configure_backward_engine(self):\n",
    "        super().configure_backward_engine_helper(\n",
    "            **self.backward_engine_model_config\n",
    "        )\n",
    "\n",
    "    def configure_teacher_generator(self):\n",
    "        super().configure_teacher_generator_helper(**self.teacher_model_config)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        to = super().configure_text_optimizer_helper(\n",
    "            **self.text_optimizer_model_config\n",
    "        )\n",
    "        do = super().configure_demo_optimizer_helper()  # Add demo optimizer\n",
    "        return to + do  # Return both text and demo optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose(\n",
    "    model_client: adal.ModelClient,\n",
    "    model_kwargs: Dict,\n",
    ") -> Dict:\n",
    "    trainset, valset, testset = (\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        test_dataset,\n",
    "    )\n",
    "    # use max_samples=10 to test the code\n",
    "\n",
    "    adal_component = ContextRelevanceAdalComponentOnTrecDL(\n",
    "        cortex_search_custom_prompt, model_client, model_kwargs\n",
    "    )\n",
    "    trainer = adal.Trainer(adaltask=adal_component)\n",
    "    trainer.diagnose(dataset=trainset, split=\"train\")\n",
    "    trainer.diagnose(dataset=valset, split=\"val\")\n",
    "    trainer.diagnose(dataset=testset, split=\"test\")\n",
    "\n",
    "\n",
    "diagnose(**az_gpt_4o_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_batch_size=4,  # larger batch size is not that effective, probably because of llm's lost in the middle\n",
    "    raw_shots: int = 0,\n",
    "    bootstrap_shots: int = 1,\n",
    "    max_steps=1,\n",
    "    num_workers=4,\n",
    "    strategy=\"random\",\n",
    "    optimization_order=\"sequential\",\n",
    "    debug=False,\n",
    "    resume_from_ckpt=None,\n",
    "    exclude_input_fields_from_bootstrap_demos=False,\n",
    "):\n",
    "    adal_component = ContextRelevanceAdalComponentOnTrecDL(\n",
    "        target_prompt=cortex_search_custom_prompt,\n",
    "        **az_gpt_4o_model,\n",
    "        teacher_model_config=az_gpt_4o_model,\n",
    "        text_optimizer_model_config=az_gpt_4o_model,\n",
    "        backward_engine_model_config=az_gpt_4o_model,\n",
    "    )\n",
    "    print(adal_component)\n",
    "    trainer = adal.Trainer(\n",
    "        train_batch_size=train_batch_size,\n",
    "        adaltask=adal_component,\n",
    "        strategy=strategy,\n",
    "        max_steps=max_steps,\n",
    "        num_workers=num_workers,\n",
    "        raw_shots=raw_shots,\n",
    "        bootstrap_shots=bootstrap_shots,\n",
    "        debug=debug,\n",
    "        weighted_sampling=True,\n",
    "        optimization_order=optimization_order,\n",
    "        exclude_input_fields_from_bootstrap_demos=exclude_input_fields_from_bootstrap_demos,\n",
    "    )\n",
    "    print(trainer)\n",
    "\n",
    "    # train_dataset, val_dataset, test_dataset = load_datasets()\n",
    "    trainer.fit(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        debug=debug,\n",
    "        resume_from_ckpt=resume_from_ckpt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    train_batch_size=4,\n",
    "    debug=False,\n",
    "    max_steps=15,\n",
    "    strategy=\"constrained\",\n",
    "    raw_shots=1,\n",
    "    bootstrap_shots=1,\n",
    "    exclude_input_fields_from_bootstrap_demos=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
