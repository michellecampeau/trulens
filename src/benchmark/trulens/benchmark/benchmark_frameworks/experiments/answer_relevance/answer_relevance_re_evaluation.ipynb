{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.dataset.beir_loader import (\n",
    "    TruBEIRDataLoader,\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(data_folder=\"./\", dataset_name=\"hotpotqa\")\n",
    "\n",
    "\n",
    "hotpotqa = beir_data_loader.load_dataset_to_df(download=True)\n",
    "\n",
    "\n",
    "hotpotqa_raw_subset = hotpotqa.sample(n=200, random_state=42)\n",
    "\n",
    "all_responses = [\n",
    "    (row[\"query\"], row[\"expected_response\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = []\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    # Positive examples for answer relevance\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": row[\"expected_response\"],  # Positive response\n",
    "        \"expected_score\": 1,  # Positive example, score = 1\n",
    "    })\n",
    "\n",
    "    # Negative examples for answer relevance (random unrelated response)\n",
    "    negative_response = random.choice([\n",
    "        r\n",
    "        for q, r in all_responses\n",
    "        if q != row[\"query\"]  # Pick response from another query\n",
    "    ])\n",
    "\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": negative_response,  # Negative response\n",
    "        \"expected_score\": 0,  # Negative example, score = 0\n",
    "    })\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_answer_relevance\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_answer_relevance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trulens.core import TruSession\n",
    "from trulens.providers.openai import AzureOpenAI\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "\n",
    "az_openai_provider = AzureOpenAI(\n",
    "    deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],  # gpt-4o\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "openai_provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def trulens_optimized_answer_relevance(\n",
    "    prompt: str, response: str, gt_score: float\n",
    ") -> str:\n",
    "    score = openai_provider.relevance(\n",
    "        prompt=prompt,\n",
    "        response=response,\n",
    "    )\n",
    "    return f\"{score};{gt_score};N/A\"\n",
    "\n",
    "\n",
    "trulens_optimized_answer_relevance(\"hey wuz good?\", \"hi im doing well\", 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Provider\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "class CustomTermFeedback(Provider):\n",
    "    def true_positive(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 1 and binary_gt_score == 1 else 0.0\n",
    "\n",
    "    def true_negative(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 0 and binary_gt_score == 0 else 0.0\n",
    "\n",
    "    def false_positive(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 1 and binary_gt_score == 0 else 0.0\n",
    "\n",
    "    def false_negative(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 0 and binary_gt_score == 1 else 0.0\n",
    "\n",
    "    def raw_gt_score(self, output: str) -> float:\n",
    "        return float(output.split(\";\")[1])\n",
    "\n",
    "    def raw_feedback_score(self, output: str) -> float:\n",
    "        return float(output.split(\";\")[0])\n",
    "\n",
    "\n",
    "custom_term_feedback = CustomTermFeedback()\n",
    "\n",
    "f_tp = Feedback(\n",
    "    custom_term_feedback.true_positive,\n",
    "    name=\"True Positive\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_tn = Feedback(\n",
    "    custom_term_feedback.true_negative,\n",
    "    name=\"True Negative\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_fp = Feedback(\n",
    "    custom_term_feedback.false_positive,\n",
    "    name=\"False Positive\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "f_fn = Feedback(\n",
    "    custom_term_feedback.false_negative,\n",
    "    name=\"False Negative\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "\n",
    "f_raw_gt_score = Feedback(\n",
    "    custom_term_feedback.raw_gt_score,\n",
    "    name=\"Raw GT Score\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_raw_feedback_score = Feedback(\n",
    "    custom_term_feedback.raw_feedback_score,\n",
    "    name=\"Raw Feedback Score\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "\n",
    "CUSTOM_FEEDBACK_FUNCS = [\n",
    "    f_tp,\n",
    "    f_tn,\n",
    "    f_fp,\n",
    "    f_fn,\n",
    "    f_raw_gt_score,\n",
    "    f_raw_feedback_score,\n",
    "]\n",
    "\n",
    "\n",
    "def run_answer_relevance_experiment(\n",
    "    func_wrapper, dataset_df, app_name, app_version=\"dataset_name\"\n",
    "):\n",
    "    tru_wrapped_app = TruBasicApp(\n",
    "        func_wrapper,\n",
    "        app_name=app_name,\n",
    "        app_version=f\"{app_version}\",\n",
    "        feedbacks=CUSTOM_FEEDBACK_FUNCS,\n",
    "    )\n",
    "\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        prompt = row[\"query\"]\n",
    "        response = row[\"expected_response\"]\n",
    "        gt_score = row[\"expected_score\"]\n",
    "\n",
    "        try:\n",
    "            with tru_wrapped_app as _:\n",
    "                tru_wrapped_app.app(prompt, response, gt_score)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with query {prompt} and response {response}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_answer_relevance_experiment(\n",
    "    trulens_optimized_answer_relevance,\n",
    "    hotpotqa_subset_for_answer_relevance,\n",
    "    \"Answer Relevance (optimized)\",\n",
    "    \"HotpotQA\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
