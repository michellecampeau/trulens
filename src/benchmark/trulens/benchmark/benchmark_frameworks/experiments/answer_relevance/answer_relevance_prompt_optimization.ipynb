{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.dataset.beir_loader import (\n",
    "    TruBEIRDataLoader,\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "beir_data_loader = TruBEIRDataLoader(data_folder=\"./\", dataset_name=\"hotpotqa\")\n",
    "\n",
    "\n",
    "hotpotqa = beir_data_loader.load_dataset_to_df(download=True)\n",
    "\n",
    "\n",
    "hotpotqa_raw_subset = hotpotqa.sample(n=200, random_state=42)\n",
    "\n",
    "all_responses = [\n",
    "    (row[\"query\"], row[\"expected_response\"])\n",
    "    for idx, row in hotpotqa_raw_subset.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = []\n",
    "\n",
    "for idx, row in hotpotqa_raw_subset.iterrows():\n",
    "    # Positive examples for answer relevance\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": row[\"expected_response\"],  # Positive response\n",
    "        \"expected_score\": 1,  # Positive example, score = 1\n",
    "    })\n",
    "\n",
    "    # Negative examples for answer relevance (random unrelated response)\n",
    "    negative_response = random.choice([\n",
    "        r\n",
    "        for q, r in all_responses\n",
    "        if q != row[\"query\"]  # Pick response from another query\n",
    "    ])\n",
    "\n",
    "    hotpotqa_subset_for_answer_relevance.append({\n",
    "        \"query\": row[\"query\"],\n",
    "        \"expected_response\": negative_response,  # Negative response\n",
    "        \"expected_score\": 0,  # Negative example, score = 0\n",
    "    })\n",
    "\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance_true_labels = [\n",
    "    entry[\"expected_score\"] for entry in hotpotqa_subset_for_answer_relevance\n",
    "]\n",
    "\n",
    "hotpotqa_subset_for_answer_relevance = pd.DataFrame(\n",
    "    hotpotqa_subset_for_answer_relevance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def balanced_split(\n",
    "    df,\n",
    "    label_column=\"expected_score\",\n",
    "    train_size=0.6,\n",
    "    dev_size=0.2,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into train, dev, and test sets with balanced labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        label_column (str): The column containing the labels to balance on.\n",
    "        train_size (float): Proportion of the data to use for training.\n",
    "        dev_size (float): Proportion of the data to use for dev/validation.\n",
    "        test_size (float): Proportion of the data to use for testing.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        train_df (pd.DataFrame): Training split.\n",
    "        dev_df (pd.DataFrame): Development/validation split.\n",
    "        test_df (pd.DataFrame): Testing split.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        abs(train_size + dev_size + test_size - 1.0) < 1e-5\n",
    "    ), \"Sizes must sum to 1.0\"\n",
    "\n",
    "    # Step 1: Split train+dev and test\n",
    "    train_dev_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        stratify=df[label_column],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Step 2: Calculate relative size for dev split from train+dev\n",
    "    dev_relative_size = dev_size / (train_size + dev_size)\n",
    "\n",
    "    # Step 3: Split train and dev\n",
    "    train_df, dev_df = train_test_split(\n",
    "        train_dev_df,\n",
    "        test_size=dev_relative_size,\n",
    "        stratify=train_dev_df[label_column],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return train_df, dev_df, test_df\n",
    "\n",
    "\n",
    "train_df, dev_df, test_df = balanced_split(\n",
    "    hotpotqa_subset_for_answer_relevance,\n",
    "    train_size=0.4,\n",
    "    dev_size=0.3,\n",
    "    test_size=0.3,\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Dev size: {len(dev_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "#     visualize_expected_score_distribution,\n",
    "# )\n",
    "\n",
    "# hotpotqa_gt_scores = [\n",
    "#     entry[\"expected_score\"] for _, entry in hotpotqa_subset_for_answer_relevance.iterrows()\n",
    "# ]\n",
    "# visualize_expected_score_distribution(hotpotqa_gt_scores)\n",
    "\n",
    "# train_scores = [entry[\"expected_score\"] for _, entry in train_df.iterrows()]\n",
    "# visualize_expected_score_distribution(train_scores)\n",
    "\n",
    "# dev_scores = [entry[\"expected_score\"] for _, entry in dev_df.iterrows()]\n",
    "# visualize_expected_score_distribution(dev_scores)\n",
    "\n",
    "# test_scores = [entry[\"expected_score\"] for _, entry in test_df.iterrows()]\n",
    "# visualize_expected_score_distribution(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement TruLens' `answer_relevance` in AdalFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback.v2.feedback import PromptResponseRelevance\n",
    "\n",
    "print(PromptResponseRelevance.system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Union\n",
    "\n",
    "import adalflow as adal\n",
    "from adalflow.optim.types import ParameterType\n",
    "import pandas as pd\n",
    "from trulens.feedback import generated as feedback_generated\n",
    "\n",
    "few_shot_template = r\"\"\"<START_OF_SYSTEM_PROMPT>\n",
    "{{system_prompt}}\n",
    "{# Few shot demos #}\n",
    "{% if few_shot_demos is not none %}\n",
    "Here are some examples:\n",
    "{{few_shot_demos}}\n",
    "{% endif %}\n",
    "<END_OF_SYSTEM_PROMPT>\n",
    "<START_OF_USER>\n",
    "{{user_prompt}}\n",
    "<END_OF_USER>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AnswerRelevanceTaskPipeline(adal.Component):\n",
    "    def __init__(self, model_client: adal.ModelClient, model_kwargs: Dict):\n",
    "        super().__init__()\n",
    "\n",
    "        system_prompt = adal.Parameter(\n",
    "            data=PromptResponseRelevance.system_prompt,\n",
    "            role_desc=\"To give task instruction to the language model in the system prompt\",\n",
    "            requires_opt=True,\n",
    "            param_type=ParameterType.PROMPT,\n",
    "            instruction_to_optimizer=\"You can try to show examples to see if it helps. Also focus on the case when prediction is 1 but ground truth is 0 (False positives)\",\n",
    "        )\n",
    "        few_shot_demos = adal.Parameter(\n",
    "            data=None,\n",
    "            role_desc=\"To provide few shot demos to the language model\",\n",
    "            requires_opt=True,  # Changed to True for few-shot learning\n",
    "            param_type=ParameterType.DEMOS,\n",
    "        )\n",
    "\n",
    "        self.evaluate_relevance = adal.Generator(\n",
    "            model_client=model_client,\n",
    "            model_kwargs=model_kwargs,\n",
    "            template=few_shot_template,\n",
    "            prompt_kwargs={\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"few_shot_demos\": few_shot_demos,\n",
    "            },\n",
    "            use_cache=True,\n",
    "            output_processors=self.parse_output,\n",
    "        )\n",
    "\n",
    "    @adal.fun_to_component\n",
    "    def parse_output(response: str):\n",
    "        score = (\n",
    "            feedback_generated.re_configured_rating(\n",
    "                response,\n",
    "                min_score_val=0,\n",
    "                max_score_val=3,\n",
    "            )\n",
    "        ) / 3\n",
    "        return score\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response: str,\n",
    "        id: Optional[str] = None,\n",
    "    ) -> Union[adal.GeneratorOutput, adal.Parameter]:\n",
    "        user_prompt = PromptResponseRelevance.user_prompt.format(\n",
    "            prompt=prompt, response=response\n",
    "        )\n",
    "\n",
    "        return self.evaluate_relevance(\n",
    "            prompt_kwargs={\"user_prompt\": user_prompt}, id=id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adalflow.components.model_client.openai_client import OpenAIClient\n",
    "\n",
    "# az_gpt_4o_model = {\n",
    "#     \"model_client\": AzureOpenAIClient(),\n",
    "#     \"model_kwargs\": {\n",
    "#         \"model\": os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "#         \"max_tokens\": 4000,\n",
    "#         \"temperature\": 0.0,\n",
    "#         \"top_p\": 0.99,\n",
    "#         \"frequency_penalty\": 0,\n",
    "#         \"presence_penalty\": 0,\n",
    "#         \"stop\": None,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "gpt_4o_model = {\n",
    "    \"model_client\": OpenAIClient(),\n",
    "    \"model_kwargs\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.99,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"stop\": None,\n",
    "    },\n",
    "}\n",
    "task_pipeline = AnswerRelevanceTaskPipeline(**gpt_4o_model)\n",
    "print(task_pipeline)\n",
    "\n",
    "output = task_pipeline(prompt=\"Is apple safe to eat?\", response=\"ha!\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_pipeline.train()  # set to train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "import uuid\n",
    "\n",
    "from adalflow.datasets.types import Example\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HotpotQAData(Example):\n",
    "    __doc__ = (\n",
    "        \"\"\"A dataclass for representing examples in the HotpotQA dataset.\"\"\"\n",
    "    )\n",
    "\n",
    "    id: str = field(\n",
    "        metadata={\"desc\": \"The unique identifier of the example\", \"type\": \"id\"},\n",
    "        default_factory=lambda: str(\n",
    "            uuid.uuid4()\n",
    "        ),  # Ensures a unique UUID for each instance\n",
    "    )\n",
    "    query: Optional[str] = field(\n",
    "        metadata={\"desc\": \"The query from user.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    expected_response: Optional[str] = field(\n",
    "        metadata={\"desc\": \"The expected answer to the query.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    expected_score: Optional[float] = field(\n",
    "        metadata={\"desc\": \"The expected relevance score for the answer.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = [\n",
    "    HotpotQAData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"],\n",
    "    )\n",
    "    for _, row in train_df.iterrows()\n",
    "]\n",
    "val_dataset = [\n",
    "    HotpotQAData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"],\n",
    "    )\n",
    "    for _, row in dev_df.iterrows()\n",
    "]\n",
    "test_dataset = [\n",
    "    HotpotQAData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"],\n",
    "    )\n",
    "    for _, row in test_df.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "def answer_relevance_eval_fn(y: float, y_gt: float) -> float:\n",
    "    return 1.0 if y == y_gt else 0.0\n",
    "\n",
    "\n",
    "def weighted_relevance_loss(\n",
    "    y: float, y_gt: float, false_negative_weight\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Penalizes false negative to improve recall and keeps the loss in [0, 1].\n",
    "    \"\"\"\n",
    "    # Identify the type of error\n",
    "    if y_gt > y:  # False negatives\n",
    "        penalty = false_negative_weight\n",
    "    elif y != y_gt and not (\n",
    "        y > 1 and y_gt == 1\n",
    "    ):  # Other mismatches (false positives)\n",
    "        penalty = 1.0\n",
    "    else:  # Correct predictions\n",
    "        return 0.0\n",
    "\n",
    "    # Normalize the penalty to keep the loss in [0, 1]\n",
    "    normalized_loss = penalty / (false_negative_weight + 1.0)\n",
    "\n",
    "    return (\n",
    "        1 - normalized_loss\n",
    "    )  # textual loss higher the better (UNLIKE typical ML loss)\n",
    "\n",
    "\n",
    "class AnswerRelevanceAdalComponent(adal.AdalComponent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_client: adal.ModelClient,\n",
    "        model_kwargs: Dict,\n",
    "        backward_engine_model_config: Dict = None,\n",
    "        teacher_model_config: Dict = None,\n",
    "        text_optimizer_model_config: Dict = None,\n",
    "    ):\n",
    "        task = AnswerRelevanceTaskPipeline(model_client, model_kwargs)\n",
    "\n",
    "        eval_fn = answer_relevance_eval_fn\n",
    "\n",
    "        loss_fn = adal.EvalFnToTextLoss(\n",
    "            eval_fn=lambda y, y_gt: weighted_relevance_loss(\n",
    "                y, y_gt, false_negative_weight=3.0\n",
    "            ),\n",
    "            eval_fn_desc=\"Give a lower score when the model gives the wrong rating comparing to ground truth to penalize both false negatives and false positives  (i.e. incorrectly classifying a relevant response as not relevant or vice versa).\",\n",
    "        )\n",
    "\n",
    "        super().__init__(task=task, eval_fn=eval_fn, loss_fn=loss_fn)\n",
    "        self.backward_engine_model_config = backward_engine_model_config\n",
    "        self.teacher_model_config = teacher_model_config\n",
    "        self.text_optimizer_model_config = text_optimizer_model_config\n",
    "\n",
    "    def prepare_task(self, sample: HotpotQAData):\n",
    "        return self.task.call, {\n",
    "            \"prompt\": sample.query,\n",
    "            \"response\": sample.expected_response,\n",
    "            \"id\": sample.id,\n",
    "        }\n",
    "\n",
    "    def prepare_loss(self, sample: HotpotQAData, pred: adal.Parameter):\n",
    "        # prepare the gt and pred for the loss function\n",
    "        y_gt = adal.Parameter(\n",
    "            name=\"y_gt\",\n",
    "            data=sample.expected_score,\n",
    "            eval_input=sample.expected_score,\n",
    "            requires_opt=False,\n",
    "        )\n",
    "\n",
    "        # print(f\"pred: {pred}\")\n",
    "        # print(f\"pred.full_response: {pred.full_response}\")\n",
    "\n",
    "        pred.eval_input = (\n",
    "            pred.full_response.data\n",
    "            if pred\n",
    "            and pred.full_response\n",
    "            and isinstance(pred.full_response.data, float)\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        return self.loss_fn, {\"kwargs\": {\"y\": pred, \"y_gt\": y_gt}}\n",
    "\n",
    "    def prepare_eval(self, sample: HotpotQAData, y_pred: adal.GeneratorOutput):\n",
    "        y_label = -1\n",
    "        if (\n",
    "            y_pred\n",
    "            and y_pred.data is not None\n",
    "            and isinstance(y_pred.data, (int, float))\n",
    "        ):\n",
    "            y_label = 1 if y_pred.data > 0.5 else 0\n",
    "        print(\n",
    "            f\"y_pred: {y_pred}, y_label: {y_label}, sample.expected_score: {sample.expected_score}\"\n",
    "        )\n",
    "        return self.eval_fn, {\"y\": y_label, \"y_gt\": sample.expected_score}\n",
    "\n",
    "    def configure_backward_engine(self):\n",
    "        super().configure_backward_engine_helper(\n",
    "            **self.backward_engine_model_config\n",
    "        )\n",
    "\n",
    "    def configure_teacher_generator(self):\n",
    "        super().configure_teacher_generator_helper(**self.teacher_model_config)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        to = super().configure_text_optimizer_helper(\n",
    "            **self.text_optimizer_model_config\n",
    "        )\n",
    "        do = super().configure_demo_optimizer_helper()  # Add demo optimizer\n",
    "        return to + do  # Return both text and demo optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose(\n",
    "    model_client: adal.ModelClient,\n",
    "    model_kwargs: Dict,\n",
    ") -> Dict:\n",
    "    trainset, valset, testset = (\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        test_dataset,\n",
    "    )\n",
    "    # use max_samples=10 to test the code\n",
    "\n",
    "    adal_component = AnswerRelevanceAdalComponent(model_client, model_kwargs)\n",
    "    trainer = adal.Trainer(adaltask=adal_component)\n",
    "    trainer.diagnose(dataset=trainset, split=\"train\")\n",
    "    trainer.diagnose(dataset=valset, split=\"val\")\n",
    "    trainer.diagnose(dataset=testset, split=\"test\")\n",
    "\n",
    "\n",
    "diagnose(**gpt_4o_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_batch_size=4,  # larger batch size is not that effective, probably because of llm's lost in the middle\n",
    "    raw_shots: int = 0,\n",
    "    bootstrap_shots: int = 1,\n",
    "    max_steps=1,\n",
    "    num_workers=4,\n",
    "    strategy=\"random\",\n",
    "    optimization_order=\"sequential\",\n",
    "    debug=False,\n",
    "    resume_from_ckpt=None,\n",
    "    exclude_input_fields_from_bootstrap_demos=False,\n",
    "):\n",
    "    adal_component = AnswerRelevanceAdalComponent(\n",
    "        **gpt_4o_model,\n",
    "        teacher_model_config=gpt_4o_model,\n",
    "        text_optimizer_model_config=gpt_4o_model,\n",
    "        backward_engine_model_config=gpt_4o_model,\n",
    "    )\n",
    "    print(adal_component)\n",
    "    trainer = adal.Trainer(\n",
    "        train_batch_size=train_batch_size,\n",
    "        adaltask=adal_component,\n",
    "        strategy=strategy,\n",
    "        max_steps=max_steps,\n",
    "        num_workers=num_workers,\n",
    "        raw_shots=raw_shots,\n",
    "        bootstrap_shots=bootstrap_shots,\n",
    "        debug=debug,\n",
    "        weighted_sampling=True,\n",
    "        optimization_order=optimization_order,\n",
    "        exclude_input_fields_from_bootstrap_demos=exclude_input_fields_from_bootstrap_demos,\n",
    "    )\n",
    "    print(trainer)\n",
    "\n",
    "    # train_dataset, val_dataset, test_dataset = load_datasets()\n",
    "    trainer.fit(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        debug=debug,\n",
    "        resume_from_ckpt=resume_from_ckpt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    train_batch_size=6,\n",
    "    debug=False,\n",
    "    max_steps=15,\n",
    "    strategy=\"constrained\",\n",
    "    raw_shots=1,\n",
    "    bootstrap_shots=1,\n",
    "    exclude_input_fields_from_bootstrap_demos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_system_prompt = \"\"\"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\nRespond only as a number from 0 to 3, where 0 is the lowest score according to the criteria and 3 is the highest possible score.\\n\\nA few additional scoring guidelines:\\n\\n- Long RESPONSES should score equally well as short RESPONSES.\\n\\n- RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3.\\n- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n- RESPONSE that is RELEVANT to none of the PROMPT should get a minimum score of 0.\\n- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 3.\\n- RESPONSE that confidently FALSE should get a score of 0.\\n- RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the least RELEVANT and get a score of 0.\\n\\n- Pay special attention to avoid false negatives by recognizing partial relevance, even if the RESPONSE is not fully aligned with the PROMPT.\\n\\n- Never elaborate.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "# az_openai_provider = AzureOpenAI(\n",
    "#     deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],  # gpt-4o\n",
    "#     api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "#     azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "#     api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "# )\n",
    "\n",
    "openai_provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def trulens_optimized_answer_relevance(\n",
    "    prompt: str, response: str, gt_score: float\n",
    ") -> str:\n",
    "    score = openai_provider.relevance(\n",
    "        prompt=prompt,\n",
    "        response=response,\n",
    "    )\n",
    "    return f\"{score};{gt_score};N/A\"\n",
    "\n",
    "\n",
    "trulens_optimized_answer_relevance(\"hey wuz good?\", \"hi im doing well\", 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa_subset_for_answer_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Provider\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "class CustomTermFeedback(Provider):\n",
    "    def true_positive(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 1 and binary_gt_score == 1 else 0.0\n",
    "\n",
    "    def true_negative(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 0 and binary_gt_score == 0 else 0.0\n",
    "\n",
    "    def false_positive(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 1 and binary_gt_score == 0 else 0.0\n",
    "\n",
    "    def false_negative(self, output: str) -> float:\n",
    "        feedback_score, gt_score = (\n",
    "            float(output.split(\";\")[0]),\n",
    "            float(output.split(\";\")[1]),\n",
    "        )\n",
    "        binary_score = 1 if feedback_score >= 0.5 else 0\n",
    "        binary_gt_score = 1 if gt_score >= THRESHOLD else 0\n",
    "        return 1.0 if binary_score == 0 and binary_gt_score == 1 else 0.0\n",
    "\n",
    "    def raw_gt_score(self, output: str) -> float:\n",
    "        return float(output.split(\";\")[1])\n",
    "\n",
    "    def raw_feedback_score(self, output: str) -> float:\n",
    "        return float(output.split(\";\")[0])\n",
    "\n",
    "\n",
    "custom_term_feedback = CustomTermFeedback()\n",
    "\n",
    "f_tp = Feedback(\n",
    "    custom_term_feedback.true_positive,\n",
    "    name=\"True Positive\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_tn = Feedback(\n",
    "    custom_term_feedback.true_negative,\n",
    "    name=\"True Negative\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_fp = Feedback(\n",
    "    custom_term_feedback.false_positive,\n",
    "    name=\"False Positive\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "f_fn = Feedback(\n",
    "    custom_term_feedback.false_negative,\n",
    "    name=\"False Negative\",\n",
    "    higher_is_better=False,\n",
    ").on_output()\n",
    "\n",
    "f_raw_gt_score = Feedback(\n",
    "    custom_term_feedback.raw_gt_score,\n",
    "    name=\"Raw GT Score\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "f_raw_feedback_score = Feedback(\n",
    "    custom_term_feedback.raw_feedback_score,\n",
    "    name=\"Raw Feedback Score\",\n",
    "    higher_is_better=True,\n",
    ").on_output()\n",
    "\n",
    "CUSTOM_FEEDBACK_FUNCS = [\n",
    "    f_tp,\n",
    "    f_tn,\n",
    "    f_fp,\n",
    "    f_fn,\n",
    "    f_raw_gt_score,\n",
    "    f_raw_feedback_score,\n",
    "]\n",
    "\n",
    "\n",
    "def run_answer_relevance_experiment(\n",
    "    func_wrapper, dataset_df, app_name, app_version=\"dataset_name\"\n",
    "):\n",
    "    tru_wrapped_app = TruBasicApp(\n",
    "        func_wrapper,\n",
    "        app_name=app_name,\n",
    "        app_version=f\"{app_version}\",\n",
    "        feedbacks=CUSTOM_FEEDBACK_FUNCS,\n",
    "    )\n",
    "\n",
    "    for i, row in dataset_df.iterrows():\n",
    "        prompt = row[\"query\"]\n",
    "        response = row[\"expected_response\"]\n",
    "        gt_score = row[\"expected_score\"]\n",
    "\n",
    "        try:\n",
    "            with tru_wrapped_app as _:\n",
    "                tru_wrapped_app.app(prompt, response, gt_score)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with query {prompt} and response {response}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_answer_relevance_experiment(\n",
    "    trulens_optimized_answer_relevance,\n",
    "    hotpotqa_subset_for_answer_relevance,\n",
    "    \"Answer Relevance (optimized)\",\n",
    "    \"HotpotQA\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
