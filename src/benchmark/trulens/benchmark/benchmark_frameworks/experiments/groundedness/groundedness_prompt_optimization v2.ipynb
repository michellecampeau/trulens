{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trulens.providers.openai import AzureOpenAI\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"sfc-cortex-analyst-dev\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://sfc-apim-sweden.azure-api.net\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"a9e754bd44684c9c820577232e188f52\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-07-01-preview\"\n",
    "az_openai_provider = AzureOpenAI(\n",
    "    deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],  # gpt-4o\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets ir_datasets\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_balanced_llm_aggrefact_benchmark,\n",
    ")\n",
    "\n",
    "llm_aggrefact_dev_df = generate_balanced_llm_aggrefact_benchmark(split=\"dev\")\n",
    "llm_aggrefact_test_df = generate_balanced_llm_aggrefact_benchmark(\n",
    "    split=\"test\"\n",
    ")  # the one used for evaluation\n",
    "llm_aggrefact_dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_aggrefact_dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_aggrefact_dev_df = llm_aggrefact_dev_df[llm_aggrefact_dev_df['dataset'] == 'AggreFact-CNN']\n",
    "# ragtruth_test_df = llm_aggrefact_test_df[llm_aggrefact_test_df['dataset'] == 'RAGTruth'] # the one selected for data slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_exact_splits_from_df(\n",
    "    df, train_size=300, dev_size=200, test_size=600, random_seed=40\n",
    "):\n",
    "    # Balance each dataset by sampling an equal number of instances per label\n",
    "    balanced_dfs = []\n",
    "    for dataset_name in df[\"dataset\"].unique():\n",
    "        df_subset = df[df[\"dataset\"] == dataset_name]\n",
    "        min_count = df_subset[\"label\"].value_counts().min()\n",
    "        df_balanced = (\n",
    "            df_subset.groupby(\"label\")\n",
    "            .apply(lambda x: x.sample(min_count, random_state=random_seed))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        balanced_dfs.append(df_balanced)\n",
    "\n",
    "    # Concatenate all balanced subsets into a single DataFrame\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "    # Ensure the dataset is large enough for the requested sizes\n",
    "    total_required_size = train_size + dev_size + test_size\n",
    "    if len(balanced_df) < total_required_size:\n",
    "        raise ValueError(\n",
    "            \"Balanced dataset size is smaller than the requested split sizes.\"\n",
    "        )\n",
    "\n",
    "    # Shuffle the data\n",
    "    balanced_df = balanced_df.sample(\n",
    "        frac=1, random_state=random_seed\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Perform explicit slicing for train, dev, and test sets\n",
    "    test_df = balanced_df[:test_size]\n",
    "    dev_df = balanced_df[test_size : test_size + dev_size]\n",
    "    train_df = balanced_df[\n",
    "        test_size + dev_size : test_size + dev_size + train_size\n",
    "    ]\n",
    "\n",
    "    return train_df, dev_df, test_df\n",
    "\n",
    "\n",
    "# Load and balance the dataset and generate splits\n",
    "# train, dev, test = generate_exact_splits_llm_aggrefact(llm_aggrefact_dev_df)\n",
    "train, dev, test = generate_exact_splits_from_df(\n",
    "    llm_aggrefact_test_df, train_size=200, dev_size=300, test_size=300\n",
    ")\n",
    "\n",
    "\n",
    "# Display sizes\n",
    "print(f\"Train size: {len(train)}\")\n",
    "print(f\"Dev size: {len(dev)}\")\n",
    "print(f\"Test size: {len(test)}\")\n",
    "\n",
    "# Optionally save to CSV\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "dev.to_csv(\"dev.csv\", index=False)\n",
    "test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "\n",
    "data_train = []\n",
    "data_dev = []\n",
    "data_test = []\n",
    "for i, example in train.iterrows():\n",
    "    data_train.append({\n",
    "        \"query\": example.doc,\n",
    "        \"expected_score\": example.label,\n",
    "        \"expected_response\": example.claim,\n",
    "    })\n",
    "\n",
    "\n",
    "for i, example in dev.iterrows():\n",
    "    data_dev.append({\n",
    "        \"query\": example.doc,\n",
    "        \"expected_score\": example.label,\n",
    "        \"expected_response\": example.claim,\n",
    "    })\n",
    "\n",
    "for i, example in test.iterrows():\n",
    "    data_test.append({\n",
    "        \"query\": example.doc,\n",
    "        \"expected_score\": example.label,\n",
    "        \"expected_response\": example.claim,\n",
    "    })\n",
    "df_train = pd.DataFrame(data_train)\n",
    "\n",
    "df_dev = pd.DataFrame(data_dev)\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "print(\n",
    "    f\"len(df_train): {len(df_train)}; len(df_dev): {len(df_dev)}; len(df_test): {len(df_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement TruLens' `groundedness_with_cot_reasons` in AdalFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.expected_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import adalflow as adal\n",
    "from adalflow.optim.types import ParameterType\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from trulens.feedback import generated as feedback_generated\n",
    "\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "\n",
    "\n",
    "few_shot_template = r\"\"\"<START_OF_SYSTEM_PROMPT>\n",
    "{{system_prompt}}\n",
    "{# Few shot demos #}\n",
    "{% if few_shot_demos is not none %}\n",
    "Here are some examples:\n",
    "{{few_shot_demos}}\n",
    "{% endif %}\n",
    "<END_OF_SYSTEM_PROMPT>\n",
    "<START_OF_USER>\n",
    "{{user_prompt}}\n",
    "<END_OF_USER>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GroundednessTaskPipeline(adal.Component):\n",
    "    def __init__(self, model_client: adal.ModelClient, model_kwargs: Dict):\n",
    "        super().__init__()\n",
    "\n",
    "        system_prompt = adal.Parameter(\n",
    "            # data=Groundedness.system_prompt,\n",
    "            data=\"\"\"\n",
    "            You are an INFORMATION OVERLAP classifier; providing the overlap of information (entailment or groundedness) between the source and statement.\n",
    "\n",
    "            Respond only as a number from 0 to 3, where 0 is the lowest score according to the criteria and 3 is the highest possible score.\\n\\nYou should score the groundedness of the statement based on the following criteria:\\n\\n- Statements that are directly supported by the source should be considered grounded and should get a high score.\\n\\n- Statements that are not directly supported by the source should be considered not grounded and should get a low score.\\n\\n- Statements of doubt, admissions of uncertainty, or not knowing the answer are considered abstention, and should be counted as the most overlap and therefore get a max score of 3.\\n\\n- Consider indirect or implicit evidence, or the context of the statement, to avoid penalizing potentially factual claims due to lack of explicit support.\\n\\n- Be cautious of false positives; ensure that high scores are only given when there is clear supporting evidence.\\n\\n- Pay special attention to cases where the prediction is 1 but the ground truth is 0, and ensure that indirect evidence is not mistaken for direct support.\\n\\nNever elaborate.\n",
    "            \"\"\",\n",
    "            role_desc=\"To give task instruction to the language model in the system prompt\",\n",
    "            requires_opt=True,\n",
    "            param_type=ParameterType.PROMPT,\n",
    "            instruction_to_optimizer=\"You can try to show examples to see if it helps. Also focus on the case when prediction is 1 but ground truth is 0 (False positives)\",\n",
    "        )\n",
    "        few_shot_demos = adal.Parameter(\n",
    "            data=None,\n",
    "            role_desc=\"To provide few shot demos to the language model\",\n",
    "            requires_opt=True,  # Changed to True for few-shot learning\n",
    "            param_type=ParameterType.DEMOS,\n",
    "        )\n",
    "\n",
    "        self.evaluate_hypothesis = adal.Generator(\n",
    "            model_client=model_client,\n",
    "            model_kwargs=model_kwargs,\n",
    "            template=few_shot_template,\n",
    "            prompt_kwargs={\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"few_shot_demos\": few_shot_demos,\n",
    "            },\n",
    "            use_cache=True,\n",
    "            output_processors=self.parse_single_groundedness_output,\n",
    "        )\n",
    "\n",
    "    @adal.fun_to_component\n",
    "    def parse_single_groundedness_output(response: str) -> Tuple[float, Dict]:\n",
    "        score, reason = None, None\n",
    "        if response and \"Supporting Evidence\" in response:\n",
    "            score = -1\n",
    "            supporting_evidence = None\n",
    "            criteria = None\n",
    "            for line in response.split(\"\\n\"):\n",
    "                if \"Score\" in line:\n",
    "                    score = (\n",
    "                        feedback_generated.re_configured_rating(\n",
    "                            line,\n",
    "                            min_score_val=0,\n",
    "                            max_score_val=3,\n",
    "                        )\n",
    "                    ) / 3\n",
    "                criteria_lines = []\n",
    "                supporting_evidence_lines = []\n",
    "                collecting_criteria = False\n",
    "                collecting_evidence = False\n",
    "\n",
    "                for line in response.split(\"\\n\"):\n",
    "                    if \"Criteria:\" in line:\n",
    "                        criteria_lines.append(\n",
    "                            line.split(\"Criteria:\", 1)[1].strip()\n",
    "                        )\n",
    "                        collecting_criteria = True\n",
    "                        collecting_evidence = False\n",
    "                    elif \"Supporting Evidence:\" in line:\n",
    "                        supporting_evidence_lines.append(\n",
    "                            line.split(\"Supporting Evidence:\", 1)[1].strip()\n",
    "                        )\n",
    "                        collecting_evidence = True\n",
    "                        collecting_criteria = False\n",
    "                    elif collecting_criteria:\n",
    "                        if \"Supporting Evidence:\" not in line:\n",
    "                            criteria_lines.append(line.strip())\n",
    "                        else:\n",
    "                            collecting_criteria = False\n",
    "                    elif collecting_evidence:\n",
    "                        if \"Criteria:\" not in line:\n",
    "                            supporting_evidence_lines.append(line.strip())\n",
    "                        else:\n",
    "                            collecting_evidence = False\n",
    "\n",
    "                criteria = \"\\n\".join(criteria_lines).strip()\n",
    "                supporting_evidence = \"\\n\".join(\n",
    "                    supporting_evidence_lines\n",
    "                ).strip()\n",
    "            reason = {\n",
    "                \"reason\": (\n",
    "                    f\"{'Criteria: ' + str(criteria)}\\n\"\n",
    "                    f\"{'Supporting Evidence: ' + str(supporting_evidence)}\"\n",
    "                )\n",
    "            }\n",
    "            score = score\n",
    "            reason = reason\n",
    "\n",
    "        else:\n",
    "            if not response:\n",
    "                score = 0\n",
    "                reason = {\"reason\": \"No response generated.\"}\n",
    "            else:\n",
    "                score = (\n",
    "                    feedback_generated.re_configured_rating(\n",
    "                        response,\n",
    "                        min_score_val=0,\n",
    "                        max_score_val=3,\n",
    "                    )\n",
    "                ) / 3\n",
    "                warnings.warn(\n",
    "                    \"No supporting evidence provided. Returning score only.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                score = score\n",
    "                reason = {}\n",
    "\n",
    "        score_pattern = re.compile(r\"Score:\\s*([0-9.]+)\")\n",
    "        match = score_pattern.search(reason.get(\"reason\", \"\"))\n",
    "        normalized_reason = None\n",
    "        if match:\n",
    "            original_reason_score = float(match.group(1))\n",
    "            normalized_reason_score = (original_reason_score) / 3\n",
    "\n",
    "            # Ensure the formatting matches exactly\n",
    "            original_string = f\"Score: {int(original_reason_score)}\"\n",
    "            replacement_string = f\"Score: {normalized_reason_score}\"\n",
    "            normalized_reason = reason.copy()\n",
    "            normalized_reason[\"reason\"] = normalized_reason[\"reason\"].replace(\n",
    "                original_string, replacement_string\n",
    "            )\n",
    "\n",
    "        if normalized_reason is not None:\n",
    "            return score, normalized_reason\n",
    "        else:\n",
    "            return score, reason\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        premise: str,\n",
    "        hypothesis: str,\n",
    "        id: Optional[str] = None,\n",
    "    ) -> Union[adal.GeneratorOutput, adal.Parameter]:\n",
    "        # TODO - add trivial statement prompt to be another parameter to optimize\n",
    "\n",
    "        # def evaluate_hypothesis(index, hypothesis):\n",
    "        user_prompt = \"\"\"SOURCE: {premise}\n",
    "\n",
    "        Hypothesis: {hypothesis}\n",
    "\n",
    "        Please answer with the template below for all statement sentences:\n",
    "\n",
    "        Criteria: <Statement Sentence>\n",
    "        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND. For the case where the statement is an abstention, say ABSTENTION>\n",
    "        Score: <Output a number based on the scoring output space / range>\n",
    "        \"\"\".format(premise=premise, hypothesis=hypothesis)\n",
    "\n",
    "        return self.evaluate_hypothesis(\n",
    "            prompt_kwargs={\"user_prompt\": user_prompt}, id=id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from adalflow.components.model_client.openai_client import AzureOpenAIClient\n",
    "\n",
    "az_gpt_4o_model = {\n",
    "    \"model_client\": AzureOpenAIClient(),\n",
    "    \"model_kwargs\": {\n",
    "        \"model\": os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.99,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"stop\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "# gpt_4o_model = {\n",
    "#     \"model_client\": OpenAIClient(),\n",
    "#     \"model_kwargs\": {\n",
    "#         \"model\": \"gpt-4o\",\n",
    "#         \"max_tokens\": 4000,\n",
    "#         \"temperature\": 0.0,\n",
    "#         \"top_p\": 0.99,\n",
    "#         \"frequency_penalty\": 0,\n",
    "#         \"presence_penalty\": 0,\n",
    "#         \"stop\": None,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "task_pipeline = GroundednessTaskPipeline(**az_gpt_4o_model)\n",
    "print(task_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = task_pipeline(\n",
    "    premise=\"All fruits are edible\", hypothesis=\" Apple is edible\"\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start auto prompt optimization with Adalflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_pipeline.train()  # set to train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "import uuid\n",
    "\n",
    "from adalflow.datasets.types import Example\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMAggreFactData(Example):\n",
    "    __doc__ = \"\"\"A dataclass for representing examples in the LLM-AggreFact dataset.\"\"\"\n",
    "\n",
    "    id: str = field(\n",
    "        metadata={\"desc\": \"The unique identifier of the example\", \"type\": \"id\"},\n",
    "        default_factory=lambda: str(\n",
    "            uuid.uuid4()\n",
    "        ),  # Ensures a unique UUID for each instance\n",
    "    )\n",
    "    query: Optional[str] = field(\n",
    "        metadata={\"desc\": \"The source context from the retrieved documents.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    expected_response: Optional[str] = field(\n",
    "        metadata={\n",
    "            \"desc\": \"The generated response to the query that its groundedness shall be evaluated.\"\n",
    "        },\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    expected_score: Optional[float] = field(\n",
    "        metadata={\"desc\": \"The expected groundedness score for the answer.\"},\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = [\n",
    "    LLMAggreFactData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"],\n",
    "    )\n",
    "    for _, row in df_train.iterrows()\n",
    "]\n",
    "val_dataset = [\n",
    "    LLMAggreFactData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"],\n",
    "    )\n",
    "    for _, row in df_dev.iterrows()\n",
    "]\n",
    "test_dataset = [\n",
    "    LLMAggreFactData(\n",
    "        query=row[\"query\"],\n",
    "        expected_response=row[\"expected_response\"],\n",
    "        expected_score=row[\"expected_score\"],\n",
    "    )\n",
    "    for _, row in df_test.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "def groundedness_eval_fn(y: float, y_gt: float) -> float:\n",
    "    y_binary = 1 if y >= 0.5 else 0\n",
    "    return 1.0 if y_binary == y_gt else 0.0\n",
    "\n",
    "\n",
    "def weighted_groundedness_loss(\n",
    "    y: float, y_gt: float, false_positive_weight: float = 3.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Penalizes false positives more heavily and keeps the loss in [0, 1].\n",
    "    \"\"\"\n",
    "    y_binary = 1 if y >= 0.5 else 0\n",
    "\n",
    "    # Identify the type of error\n",
    "    if y_binary == 1 and y_gt == 0:  # False positive\n",
    "        penalty = false_positive_weight\n",
    "    elif y_binary != y_gt:  # Other mismatches (false negatives)\n",
    "        penalty = 1.0\n",
    "    else:  # Correct predictions\n",
    "        return 0.0\n",
    "\n",
    "    # Normalize the penalty to keep the loss in [0, 1]\n",
    "    normalized_loss = penalty / (false_positive_weight + 1.0)\n",
    "\n",
    "    return normalized_loss\n",
    "\n",
    "\n",
    "class GroundednessAdalComponentOnLLMAggreFact(adal.AdalComponent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_client: adal.ModelClient,\n",
    "        model_kwargs: Dict,\n",
    "        backward_engine_model_config: Dict = None,\n",
    "        teacher_model_config: Dict = None,\n",
    "        text_optimizer_model_config: Dict = None,\n",
    "    ):\n",
    "        task = GroundednessTaskPipeline(model_client, model_kwargs)\n",
    "        # eval_fn = AnswerMatchAcc(type=\"exact_match\").compute_single_item\n",
    "        eval_fn = groundedness_eval_fn\n",
    "        loss_fn = adal.EvalFnToTextLoss(\n",
    "            eval_fn=lambda y, y_gt: weighted_groundedness_loss(\n",
    "                y, y_gt, false_positive_weight=3.0\n",
    "            ),\n",
    "            eval_fn_desc=(\n",
    "                \"Weighted loss to penalize false positives - this is when the model classifies the claim as factual while it's not fully supported by the source document: \"\n",
    "                \"\"\"y_binary = 1 if y >= 0.5 else 0\n",
    "                    if y_binary == 1 and y_gt == 0:  # False positive\n",
    "                        penalty = false_positive_weight\n",
    "                    elif y_binary != y_gt:  # Other mismatches (false negatives)\n",
    "                        penalty = 1.0\n",
    "                    else:  # Correct predictions\n",
    "                        return 0.0\n",
    "\n",
    "                    # Normalize the penalty to keep the loss in [0, 1]\n",
    "                    normalized_loss = penalty / (false_positive_weight + 1.0)\n",
    "\n",
    "                    return normalized_loss\n",
    "                \"\"\"\n",
    "            ),\n",
    "        )\n",
    "        # loss_fn = adal.EvalFnToTextLoss(\n",
    "        #     eval_fn=eval_fn,\n",
    "        #     eval_fn_desc=\"exact_match: 1 if y == y_gt else 0\",\n",
    "        # )\n",
    "\n",
    "        super().__init__(task=task, eval_fn=eval_fn, loss_fn=loss_fn)\n",
    "        self.backward_engine_model_config = backward_engine_model_config\n",
    "        self.teacher_model_config = teacher_model_config\n",
    "        self.text_optimizer_model_config = text_optimizer_model_config\n",
    "\n",
    "    def prepare_task(self, sample: LLMAggreFactData):\n",
    "        return self.task.call, {\n",
    "            \"premise\": sample.query,\n",
    "            \"hypothesis\": sample.expected_response,\n",
    "            \"id\": sample.id,\n",
    "        }\n",
    "\n",
    "    def prepare_loss(self, sample: LLMAggreFactData, pred: adal.Parameter):\n",
    "        # prepare the gt and pred for the loss function\n",
    "        y_gt = adal.Parameter(\n",
    "            name=\"y_gt\",\n",
    "            data=sample.expected_score,\n",
    "            eval_input=sample.expected_score,\n",
    "            requires_opt=False,\n",
    "        )\n",
    "\n",
    "        pred.eval_input = (\n",
    "            pred.full_response.data[0]\n",
    "            if pred and pred.full_response and len(pred.full_response.data) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        return self.loss_fn, {\"kwargs\": {\"y\": pred, \"y_gt\": y_gt}}\n",
    "\n",
    "    def prepare_eval(\n",
    "        self, sample: LLMAggreFactData, y_pred: adal.GeneratorOutput\n",
    "    ):\n",
    "        # print(\"ok printing prepare eval\")\n",
    "\n",
    "        # print(f\"Y_pred: {y_pred}\")\n",
    "\n",
    "        y_label = -1\n",
    "        if (\n",
    "            y_pred\n",
    "            and y_pred.data\n",
    "            and len(y_pred.data) > 0\n",
    "            and isinstance(y_pred.data[0], float)\n",
    "        ):\n",
    "            y_label = y_pred.data[0]\n",
    "        return self.eval_fn, {\"y\": y_label, \"y_gt\": sample.expected_score}\n",
    "\n",
    "    def configure_backward_engine(self):\n",
    "        super().configure_backward_engine_helper(\n",
    "            **self.backward_engine_model_config\n",
    "        )\n",
    "\n",
    "    def configure_teacher_generator(self):\n",
    "        super().configure_teacher_generator_helper(**self.teacher_model_config)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        to = super().configure_text_optimizer_helper(\n",
    "            **self.text_optimizer_model_config\n",
    "        )\n",
    "        do = super().configure_demo_optimizer_helper()  # Add demo optimizer\n",
    "        return to + do  # Return both text and demo optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose(\n",
    "    model_client: adal.ModelClient,\n",
    "    model_kwargs: Dict,\n",
    ") -> Dict:\n",
    "    trainset, valset, testset = (\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        test_dataset,\n",
    "    )\n",
    "    # use max_samples=10 to test the code\n",
    "\n",
    "    adal_component = GroundednessAdalComponentOnLLMAggreFact(\n",
    "        model_client, model_kwargs\n",
    "    )\n",
    "    trainer = adal.Trainer(adaltask=adal_component)\n",
    "    trainer.diagnose(dataset=trainset, split=\"train\")\n",
    "    trainer.diagnose(dataset=valset, split=\"val\")\n",
    "    trainer.diagnose(dataset=testset, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnose(**az_gpt_4o_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_batch_size=2,  # larger batch size is not that effective, probably because of llm's lost in the middle\n",
    "    raw_shots: int = 0,\n",
    "    bootstrap_shots: int = 2,\n",
    "    max_steps=1,\n",
    "    num_workers=4,\n",
    "    strategy=\"random\",\n",
    "    optimization_order=\"sequential\",\n",
    "    debug=False,\n",
    "    resume_from_ckpt=None,\n",
    "    exclude_input_fields_from_bootstrap_demos=False,\n",
    "):\n",
    "    adal_component = GroundednessAdalComponentOnLLMAggreFact(\n",
    "        **az_gpt_4o_model,\n",
    "        teacher_model_config=az_gpt_4o_model,\n",
    "        text_optimizer_model_config=az_gpt_4o_model,\n",
    "        backward_engine_model_config=az_gpt_4o_model,\n",
    "    )\n",
    "    print(adal_component)\n",
    "    trainer = adal.Trainer(\n",
    "        train_batch_size=train_batch_size,\n",
    "        adaltask=adal_component,\n",
    "        strategy=strategy,\n",
    "        max_steps=max_steps,\n",
    "        num_workers=num_workers,\n",
    "        raw_shots=raw_shots,\n",
    "        bootstrap_shots=bootstrap_shots,\n",
    "        debug=debug,\n",
    "        weighted_sampling=True,\n",
    "        optimization_order=optimization_order,\n",
    "        exclude_input_fields_from_bootstrap_demos=exclude_input_fields_from_bootstrap_demos,\n",
    "    )\n",
    "    print(trainer)\n",
    "\n",
    "    # train_dataset, val_dataset, test_dataset = load_datasets()\n",
    "    trainer.fit(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        debug=debug,\n",
    "        resume_from_ckpt=resume_from_ckpt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    train_batch_size=8,\n",
    "    debug=False,\n",
    "    max_steps=8,\n",
    "    strategy=\"constrained\",\n",
    "    raw_shots=1,\n",
    "    bootstrap_shots=1,\n",
    "    exclude_input_fields_from_bootstrap_demos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function to plot a confusion matrix\n",
    "def plot_confusion_matrix(tp, tn, fp, fn, title):\n",
    "    matrix = [[tp, fn], [fp, tn]]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    cax = ax.matshow(matrix, cmap=\"Blues\")\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels([\"Positive\", \"Negative\"])\n",
    "    ax.set_yticklabels([\"Positive\", \"Negative\"])\n",
    "\n",
    "    for (i, j), val in np.ndenumerate(matrix):\n",
    "        ax.text(\n",
    "            j, i, f\"{val}\", ha=\"center\", va=\"center\", color=\"black\", fontsize=12\n",
    "        )\n",
    "\n",
    "    plt.title(title, pad=20)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "csv_file = (\n",
    "    \"/Users/dhuang/Documents/git/trulens/LLM AggreFact groundedness (2).csv\"\n",
    ")\n",
    "# Convert data to DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "# Aggregate TP, TN, FP, FN across all rows\n",
    "total_tp = df[\"TOTAL_TP\"].sum()\n",
    "total_tn = df[\"TOTAL_TN\"].sum()\n",
    "total_fp = df[\"TOTAL_FP\"].sum()\n",
    "total_fn = df[\"TOTAL_FN\"].sum()\n",
    "\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    total_tp,\n",
    "    total_tn,\n",
    "    total_fp,\n",
    "    total_fn,\n",
    "    \"Confusion Matrix (before optimization)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file2 = (\n",
    "    \"/Users/dhuang/Documents/git/trulens/Optimized prompt - LLM AggreFact.csv\"\n",
    ")\n",
    "# Convert data to DataFrame\n",
    "df = pd.read_csv(csv_file2)\n",
    "\n",
    "\n",
    "# Aggregate TP, TN, FP, FN across all rows\n",
    "total_tp = df[\"TOTAL_TP\"].sum()\n",
    "total_tn = df[\"TOTAL_TN\"].sum()\n",
    "total_fp = df[\"TOTAL_FP\"].sum()\n",
    "total_fn = df[\"TOTAL_FN\"].sum()\n",
    "\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    total_tp,\n",
    "    total_tn,\n",
    "    total_fp,\n",
    "    total_fn,\n",
    "    \"Confusion Matrix (after optimization)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
