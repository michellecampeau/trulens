{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Comprehensiveness Evaluations\n",
    "\n",
    "In many ways, feedbacks can be thought of as LLM apps themselves. Given text,\n",
    "they return some result. Thinking in this way, we can use TruLens to evaluate\n",
    "and track our feedback quality. We can even do this for different models (e.g. \n",
    "gpt-4o) or prompting schemes (such as chain-of-thought reasoning).\n",
    "\n",
    "This notebook follows an evaluation of a set of test cases generated from human\n",
    "annotated datasets. In particular, we generate test cases from\n",
    "[MeetingBank](https://arxiv.org/abs/2305.17529) to evaluate our\n",
    "comprehensiveness feedback function.\n",
    "\n",
    "MeetingBank is one of the datasets dedicated to automated evaluations on\n",
    "summarization tasks, which are closely related to the comprehensiveness\n",
    "evaluation in RAG with the retrieved context (i.e. the source) and response\n",
    "(i.e. the summary). It contains human annotation of numerical score (**1** to\n",
    "**5**). \n",
    "\n",
    "For evaluating comprehensiveness feedback functions, we compute the annotated\n",
    "\"informativeness\" scores, a measure of how well  the summaries capture all the\n",
    "main points of the meeting segment. A good summary should contain all and only\n",
    "the important information of the source., and normalized to **0** to **1** score\n",
    "as our **expected_score** and to match the output of feedback functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI as fOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_cases import generate_meetingbank_comprehensiveness_benchmark\n",
    "\n",
    "test_cases_gen = generate_meetingbank_comprehensiveness_benchmark(\n",
    "    human_annotation_file_path=\"./datasets/meetingbank/human_scoring.json\",\n",
    "    meetingbank_file_path=\"YOUR_LOCAL_DOWNLOAD_PATH/MeetingBank/Metadata/MeetingBank.json\",\n",
    ")\n",
    "length = sum(1 for _ in test_cases_gen)\n",
    "test_cases_gen = generate_meetingbank_comprehensiveness_benchmark(\n",
    "    human_annotation_file_path=\"./datasets/meetingbank/human_scoring.json\",\n",
    "    meetingbank_file_path=\"YOUR_LOCAL_DOWNLOAD_PATH/MeetingBank/Metadata/MeetingBank.json\",\n",
    ")\n",
    "\n",
    "comprehensiveness_golden_set = []\n",
    "for i in range(length):\n",
    "    comprehensiveness_golden_set.append(next(test_cases_gen))\n",
    "\n",
    "assert len(comprehensiveness_golden_set) == length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensiveness_golden_set[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # for groundtruth feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = TruSession()\n",
    "\n",
    "provider_gpt_4o = fOpenAI(model_engine=\"gpt-4o\")\n",
    "\n",
    "provider_gpt_4o_mini = fOpenAI(model_engine=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprehensiveness of summary with transcript as reference\n",
    "\n",
    "f_comprehensiveness_openai_gpt_4o = Feedback(\n",
    "    provider_gpt_4o.comprehensiveness_with_cot_reasons\n",
    ").on_input_output()\n",
    "\n",
    "f_comprehensiveness_openai_gpt_4o_mini = Feedback(\n",
    "    provider_gpt_4o_mini.comprehensiveness_with_cot_reasons\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Feedback object using the numeric_difference method of the\n",
    "# ground_truth object.\n",
    "ground_truth = GroundTruthAgreement(\n",
    "    comprehensiveness_golden_set, provider=fOpenAI()\n",
    ")\n",
    "\n",
    "# Call the numeric_difference method with app and record and aggregate to get\n",
    "# the mean absolute error.\n",
    "f_mae = (\n",
    "    Feedback(ground_truth.absolute_error, name=\"Mean Absolute Error\")\n",
    "    .on(Select.Record.calls[0].args.args[0])\n",
    "    .on(Select.Record.calls[0].args.args[1])\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gpt_4o = []\n",
    "scores_gpt_4o_mini = []\n",
    "true_scores = []  # human prefrences / scores\n",
    "\n",
    "for i in range(190, len(comprehensiveness_golden_set)):\n",
    "    source = comprehensiveness_golden_set[i][\"query\"]\n",
    "    summary = comprehensiveness_golden_set[i][\"response\"]\n",
    "    expected_score = comprehensiveness_golden_set[i][\"expected_score\"]\n",
    "\n",
    "    feedback_score_gpt_4o = f_comprehensiveness_openai_gpt_4o(source, summary)[\n",
    "        0\n",
    "    ]\n",
    "    feedback_score_gpt_4o_mini = f_comprehensiveness_openai_gpt_4o_mini(\n",
    "        source, summary\n",
    "    )[0]\n",
    "\n",
    "    scores_gpt_4o.append(feedback_score_gpt_4o)\n",
    "    scores_gpt_4o_mini.append(feedback_score_gpt_4o_mini)\n",
    "    true_scores.append(expected_score)\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "        \"scores (gpt-4o)\": scores_gpt_4o,\n",
    "        \"scores (gpt-4o-mini)\": scores_gpt_4o_mini,\n",
    "        \"expected score\": true_scores,\n",
    "    })\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df_results.to_csv(\n",
    "        \"./results/results_comprehensiveness_benchmark_new_3.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_gpt_4o = sum(\n",
    "    abs(score - true_score)\n",
    "    for score, true_score in zip(scores_gpt_4o, true_scores)\n",
    ") / len(scores_gpt_4o)\n",
    "\n",
    "mae_gpt_4o_mini = sum(\n",
    "    abs(score - true_score)\n",
    "    for score, true_score in zip(scores_gpt_4o_mini, true_scores)\n",
    ") / len(scores_gpt_4o_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MAE gpt-4o: {mae_gpt_4o}\")\n",
    "print(f\"MAE gpt-4o-mini: {mae_gpt_4o_mini}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization to help investigation in LLM alignments with (mean) absolute errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming scores and true_scores are flat lists of predicted probabilities and\n",
    "# their corresponding ground truth relevances\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = np.abs(np.array(scores_gpt_4o) - np.array(true_scores))\n",
    "\n",
    "# Scatter plot of scores vs true_scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# First subplot: scatter plot with color-coded errors\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(scores_gpt_4o, true_scores, c=errors, cmap=\"viridis\")\n",
    "plt.colorbar(scatter, label=\"Absolute Error\")\n",
    "plt.plot(\n",
    "    [0, 1], [0, 1], \"r--\", label=\"Perfect Alignment\"\n",
    ")  # Line of perfect alignment\n",
    "plt.xlabel(\"Model Scores\")\n",
    "plt.ylabel(\"True Scores\")\n",
    "plt.title(\"Model (GPT-4o) Scores vs. True Scores\")\n",
    "plt.legend()\n",
    "\n",
    "# Second subplot: Error across score ranges\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(scores_gpt_4o, errors, color=\"blue\")\n",
    "plt.xlabel(\"Model Scores\")\n",
    "plt.ylabel(\"Absolute Error\")\n",
    "plt.title(\"Error Across Score Ranges\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
